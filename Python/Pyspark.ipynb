{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark ---- Version 2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark context\n",
    "A SparkContext represents the entry point to Spark functionality. It's like a key to your car. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "The version of Spark Context in the PySpark shell is 2.2.0\n",
      "The Python version of Spark Context in the PySpark shell is 3.7\n",
      "The master of Spark Context in the PySpark shell is local[*]\n",
      "The app name of Spark Context in the PySpark is myApp\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "from pyspark.sql import SQLContext\n",
    "# Print the version of SparkContext\n",
    "\n",
    "#A SparkContext represents the entry point to Spark functionality.\n",
    "#It's like a key to your car. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc.\n",
    "sc = SparkContext(appName = 'myApp')\n",
    "\n",
    "#class pyspark.sql.SparkSession(sparkContext, jsparkSession=None)[source]¶\n",
    "#The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "\n",
    "#A SparkSession can be used create DataFrame, register DataFrame as tables, \n",
    "#execute SQL over tables, cache tables, and read parquet files. \n",
    "#To create a SparkSession, use the following builder pattern:\n",
    "#local[*] specifies the use of all cores available on the local machine\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
    "\n",
    "print('The app name of Spark Context in the PySpark is', sc.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive use of pyspark shell\n",
    "PySpark shell is an interactive shell for basic testing and debugging but it is quite powerful. The easiest way to demonstrate the power of PySpark’s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.\n",
    "The most important thing to understand here is that we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell.\n",
    "Instructions\n",
    "Create a python list named numb containing the numbers 1 to 100.\n",
    "Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data.\n",
    "\n",
    "### RDDs from Parallelized Collections\n",
    "Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.\n",
    "Remember you already have a SparkContext sc available in your workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code\n",
    "# Create a python list of numbers from 1 to 100 \n",
    "numb = range(1, 100)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb,30)#use parallelize() create a RDD, the elements in the list are partitioned automatically, and sent to different machine.\n",
    "#spark = SparkSession(sc)\n",
    "spark_data\n",
    "spark_data.getNumPartitions()#check how many part split\n",
    "\n",
    "\n",
    "#spark_data.glom().collect()# if a 4-core laptop, will be 4. This is dangerous method for memory. donn't use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LEARNING ABOUT RDD!\n",
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))\n",
    "RDD.getNumPartitions()\n",
    "\n",
    "## Print the file_path\n",
    "#print(\"The file_path is\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs from external dataset\n",
    "PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (file_path) with the file name README.md which is already available in your workspace.\n",
    "Remember you already have a SparkContext sc available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checl the file type: <class 'pyspark.rdd.RDD'>\n",
      "Check The number of partitions in fileRDD:\n",
      "The number of partitions is: 2\n",
      "Number of partitions in fileRDD_part is 5\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "729\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#known file path\n",
    "#Create a fileRDD from file_path, but with the method textFile because it takes the filepath instead of the direct data\n",
    "file_path = './shakespeare.txt'\n",
    "fileRDD = sc.textFile(file_path)\n",
    "print('checl the file type:', type(fileRDD))\n",
    "\n",
    "print('Check The number of partitions in fileRDD:')\n",
    "print('The number of partitions is:',fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\n",
    "\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda, filter, and map\n",
    "\n",
    "#### Review 1. Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input list: [2, 3, 4, 5, 6, 7, 8, 9, 9, 0]\n",
      "The squared numbers are: [4, 9, 16, 25, 36, 49, 64, 81, 81, 0]\n",
      "Input list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
      "Numbers divisible by 10 are: [10, 40, 60, 80]\n"
     ]
    }
   ],
   "source": [
    "### using lambda, filter and map function\n",
    "my_list = [2,3,4,5,6,7,8,9,9,0]\n",
    "print('input list:', my_list)\n",
    "\n",
    "squared_list = list(map(lambda x: x**2, my_list))\n",
    "print('The squared numbers are:',squared_list)\n",
    "\n",
    "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "filterd_list = list(filter(lambda x: (x%10==0), my_list2))\n",
    "print(\"Numbers divisible by 10 are:\", filterd_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method by which you can manipulate data is PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use map() transformation to cube each number of the numbRDD RDD that you created earlier. Next, you'll return all the elements to a variable and finally print the output. \n",
    "\n",
    "The RDD transformation filter() returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the README.md file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD.\n",
    "\n",
    "Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.\n",
    "Remember, you already have a SparkContext sc, and numbRDD available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "729\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#my note:check the members in numbRDD.\n",
    "#for numb in numbRDD.map(lambda x:x).collect():\n",
    "    #print(numb)\n",
    "\n",
    "#Create map() transformation to cube numbers\n",
    "numbRDD = sc.parallelize(my_list)\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect() # number_all type is list\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)\n",
    "\n",
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "#fileRDD_filter = fileRDD.filter(lambda line: 'spark' in line)\n",
    "\n",
    "#How many lines are there in fileRDD?\n",
    "#print('The total number of lines containing spark is:',fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "#for line in fileRDD.filter.take(4):\n",
    "#    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by key \n",
    "One of the most popular pair RDD transformations is reduceByKey() which operates on key, value (k,v) pairs and merges the values for each key. In this exercise, you'll first create a pair RDD from a list of tuples, then combine the values with the same key and finally print out the result.\n",
    "Remember, you already have a SparkContext sc available in your workspace.\n",
    "\n",
    "\n",
    "Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).\n",
    "Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.\n",
    "Collect the contents of pair RDD Rdd_Reduced and iterate to print the output.\n",
    "\n",
    "### Sort by key\n",
    "Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later in the chapter). In this exercise, you'll sort the pair RDD Rdd_Reduced that you created in the previous exercise into descending order and print the final output.\n",
    "\n",
    "Sort the Rdd_Reduced RDD using the key in descending order.\n",
    "Collect the contents and iterate to print the output\n",
    "\n",
    "### Count by key\n",
    "Use the countByKey() action on the Rdd to count the unique keys and assign the result to a variable total.\n",
    "What is the type of total?\n",
    "Iterate over the total and print the keys and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 12), (7, 8)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(1,4),(1,6),(7,8)])\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_reduced = Rdd.reduceByKey(lambda x,y: x+y)\n",
    "Rdd_reduced.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rdd_Reduced_Sort = Rdd.sortByKey(ascending=False)\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 7 has 8 Counts\n",
      "Key 1 has 2 Counts\n",
      "Key 1 has 4 Counts\n",
      "Key 1 has 6 Counts\n"
     ]
    }
   ],
   "source": [
    "Rdd_Reduced_Sort = Rdd.sortByKey(ascending=False)\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "Rdd_Reduced_Sort.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 2 counts\n",
      "key 2 has 1 counts\n",
      "key 7 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "Rdd1 = sc.parallelize([(1,2),(1,4),(2,6),(7,8)])\n",
    "total = Rdd1.countByKey()\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from Complete Works of William Shakespeare.\n",
    "\n",
    "Here are the brief steps for writing the word counting program:\n",
    "\n",
    "Create a base RDD from Complete_Shakespeare.txt file.\n",
    "\n",
    "Use RDD transformation to create a long list of words from each element of the base RDD.\n",
    "Remove stop words from your data.\n",
    "\n",
    "Create pair RDD where each element is a pair tuple of ('w', 1)\n",
    "\n",
    "Group the elements of the pair RDD by key (word) and add up their values.\n",
    "\n",
    "Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
    "\n",
    "Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
    "\n",
    "In this first exercise, you'll create a base RDD from Complete_Shakespeare.txt file and transform it to create a long list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 1418390\n",
      "('Etext', 4)\n",
      "('presented', 11)\n",
      "('Project', 13)\n",
      "('Gutenberg,', 1)\n",
      "('cooperation', 1)\n",
      "('World', 5)\n",
      "('Library,', 2)\n",
      "('Inc.,', 1)\n",
      "('Future', 3)\n",
      "('Shakespeare', 45)\n",
      " has 517065 counts\n",
      "thou has 4247 counts\n",
      "thy has 3630 counts\n",
      "shall has 3016 counts\n",
      "good has 2046 counts\n",
      "would has 1974 counts\n",
      "Enter has 1926 counts\n",
      "thee has 1780 counts\n",
      "I'll has 1737 counts\n",
      "hath has 1614 counts\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.corpus import stopwords\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile('./shakespeare.txt')\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w,1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending = False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "* RDD To Dataframe\n",
    "* CSV To Dataframee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RDD To Dataframe\n",
    "Similar to RDDs, DataFrames are immutable and distributed data structures in Spark. Even though RDDs are a fundamen\n",
    "tal data structure in Spark, working with data in DataFrame is easier than RDD most of the time and so understanding of how to convert RDD to DataFrame is necessary.\n",
    "\n",
    "In this exercise, you'll first make an RDD using the sample_list which contains the list of tuples ('Mona',20), ('Jennifer',34),('John',20), ('Jim',26) with each tuple contains the name of the person and their age. Next, you'll create a DataFrame using the RDD and the schema (which is the list of 'Name' and 'Age') and finally confirm the output as PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "rdd.collect()\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd.collect(), ['name', 'age'])\n",
    "# Check the type of people_df\n",
    "print(\"The type of names_df is\", type(names_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [('Alice', 1)]\n",
    "spark.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(l).collect()\n",
    "spark.createDataFrame(rdd,['name', 'age']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load csv into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of people_df is <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+-------+---------+----------+----------+-------------+---------+----------+-----------------+----------+\n",
      "|_c0|ZipCode| Latitude| Longitude|      City| StateGeoName|      lat|       lon|         city_lat|  city_lon|\n",
      "+---+-------+---------+----------+----------+-------------+---------+----------+-----------------+----------+\n",
      "|  0|    501|40.813078|-73.046388|Holtsville|     New York|40.813078|-73.046388|40.81230433333334|-73.046507|\n",
      "|  1|    544|40.813223|-73.049288|Holtsville|     New York|40.813223|-73.049288|40.81230433333334|-73.046507|\n",
      "|  2|   1001|42.071523|-72.624257|    Agawam|Massachusetts|42.071523|-72.624257|        42.071523|-72.624257|\n",
      "|  3|   1001|42.071523|-72.624257|    Agawam|Massachusetts|42.071523|-72.624257|        42.071523|-72.624257|\n",
      "|  4|   1002| 42.37686| -72.46914|   Amherst|Massachusetts| 42.37686| -72.46914|42.39104714285714|-72.506747|\n",
      "|  5|   1002| 42.37686| -72.46914|   Amherst|Massachusetts| 42.37686| -72.46914|42.39104714285714|-72.506747|\n",
      "|  6|   1003| 42.40524|-72.528427|   Amherst|Massachusetts| 42.40524|-72.528427|42.39104714285714|-72.506747|\n",
      "|  7|   1003| 42.40524|-72.528427|   Amherst|Massachusetts| 42.40524|-72.528427|42.39104714285714|-72.506747|\n",
      "|  8|   1003| 42.40524|-72.528427|   Amherst|Massachusetts| 42.40524|-72.528427|42.39104714285714|-72.506747|\n",
      "|  9|   1004|42.383945|-72.511834|   Amherst|Massachusetts|42.383945|-72.511834|42.39104714285714|-72.506747|\n",
      "+---+-------+---------+----------+----------+-------------+---------+----------+-----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There are 68072 rows in the people_df DataFrame.\n",
      "There are 10 columns in the people_df DataFrame and their names are ['_c0', 'ZipCode', 'Latitude', 'Longitude', 'City', 'StateGeoName', 'lat', 'lon', 'city_lat', 'city_lon']\n"
     ]
    }
   ],
   "source": [
    "# Create an DataFrame from file_path\n",
    "file_path = 'tczips.csv'\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))\n",
    "people_df.show(10)\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+---------+\n",
      "|ZipCode|      City| StateGeoName|      lat|\n",
      "+-------+----------+-------------+---------+\n",
      "|    501|Holtsville|     New York|40.813078|\n",
      "|    544|Holtsville|     New York|40.813223|\n",
      "|   1001|    Agawam|Massachusetts|42.071523|\n",
      "|   1001|    Agawam|Massachusetts|42.071523|\n",
      "|   1002|   Amherst|Massachusetts| 42.37686|\n",
      "|   1002|   Amherst|Massachusetts| 42.37686|\n",
      "|   1003|   Amherst|Massachusetts| 42.40524|\n",
      "|   1003|   Amherst|Massachusetts| 42.40524|\n",
      "|   1003|   Amherst|Massachusetts| 42.40524|\n",
      "|   1004|   Amherst|Massachusetts|42.383945|\n",
      "+-------+----------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There were 68072 rows before removing duplicates, and 42509 rows after removing duplicates\n"
     ]
    }
   ],
   "source": [
    "people_df_sub = people_df.select('ZipCode','City','StateGeoName','lat')\n",
    "people_df_sub.show(10)\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 281 rows in the people_df_NY DataFrame and 20 rows in the people_df_AM DataFrame\n"
     ]
    }
   ],
   "source": [
    "people_df_NY = people_df.filter(people_df.City == 'New York')\n",
    "people_df_AM = people_df.filter(people_df.City == 'Amherst')\n",
    "print(\"There are {} rows in the people_df_NY DataFrame and {} rows in the people_df_AM DataFrame\".format(people_df_NY.count(), people_df_AM.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      City|\n",
      "+----------+\n",
      "|Holtsville|\n",
      "|Holtsville|\n",
      "|    Agawam|\n",
      "|    Agawam|\n",
      "|   Amherst|\n",
      "|   Amherst|\n",
      "|   Amherst|\n",
      "|   Amherst|\n",
      "|   Amherst|\n",
      "|   Amherst|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView('people')\n",
    "# Construct a \"query\" to select the names of the people\n",
    "query = '''SELECT City FROM people'''\n",
    "people_df_city = spark.sql(query)\n",
    "people_df_city.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+----------+--------+------------+---------+----------+-----------------+------------------+\n",
      "| _c0|ZipCode| Latitude| Longitude|    City|StateGeoName|      lat|       lon|         city_lat|          city_lon|\n",
      "+----+-------+---------+----------+--------+------------+---------+----------+-----------------+------------------+\n",
      "|5251|  10001| 40.75205|-73.994517|New York|    New York| 40.75205|-73.994517|40.75074491459075|-73.98446786476867|\n",
      "|5252|  10001| 40.75205|-73.994517|New York|    New York| 40.75205|-73.994517|40.75074491459075|-73.98446786476867|\n",
      "|5253|  10001| 40.75205|-73.994517|New York|    New York| 40.75205|-73.994517|40.75074491459075|-73.98446786476867|\n",
      "|5254|  10001| 40.75205|-73.994517|New York|    New York| 40.75205|-73.994517|40.75074491459075|-73.98446786476867|\n",
      "|5255|  10002|40.715523|-73.988379|New York|    New York|40.715523|-73.988379|40.75074491459075|-73.98446786476867|\n",
      "|5256|  10002|40.715523|-73.988379|New York|    New York|40.715523|-73.988379|40.75074491459075|-73.98446786476867|\n",
      "|5257|  10003|40.732373|-73.989477|New York|    New York|40.732373|-73.989477|40.75074491459075|-73.98446786476867|\n",
      "|5258|  10003|40.732373|-73.989477|New York|    New York|40.732373|-73.989477|40.75074491459075|-73.98446786476867|\n",
      "|5259|  10003|40.732373|-73.989477|New York|    New York|40.732373|-73.989477|40.75074491459075|-73.98446786476867|\n",
      "|5260|  10004|40.704169|-74.012311|New York|    New York|40.704169|-74.012311|40.75074491459075|-73.98446786476867|\n",
      "+----+-------+---------+----------+--------+------------+---------+----------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df_NY = spark.sql(\"SELECT * FROM people WHERE City == 'New York'\")\n",
    "people_df_NY.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- ZipCode: integer (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- StateGeoName: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- city_lat: double (nullable = true)\n",
      " |-- city_lon: double (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------------+-----------------+------------------+----------+------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|              _c0|           ZipCode|         Latitude|         Longitude|      City|StateGeoName|              lat|               lon|          city_lat|          city_lon|\n",
      "+-------+-----------------+------------------+-----------------+------------------+----------+------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|            68072|             68072|            68070|             68070|     68072|       68072|            68070|             68070|             68072|             68072|\n",
      "|   mean|          34035.5| 46068.84492889881|38.95510762803024|-89.29875026246486|      null|        null|38.95510762803024|-89.29875026246486|38.954878813607515|-89.29899923266505|\n",
      "| stddev|19650.83809917531|25973.505380525312| 5.19493364940427|14.233950170926175|      null|        null| 5.19493364940427|14.233950170926175|  5.19496432619849|14.233768188969897|\n",
      "|    min|                0|               501|        19.061249|       -176.658889|Aaronsburg|     Alabama|        19.061249|       -176.658889|         19.061249|       -176.658889|\n",
      "|    max|            68071|             99950|         70.66976|        -66.991337|    Zwolle|     Wyoming|         70.66976|        -66.991337|          70.66976|        -66.991337|\n",
      "+-------+-----------------+------------------+-----------------+------------------+----------+------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_pandas = people_df.toPandas()\n",
    "people_df.printSchema()\n",
    "people_df.describe().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "# $example off$\n",
    "from pyspark import SparkContext\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"Ranking Metrics Example\")\n",
    "\n",
    "    # Several of the methods available in scala are currently missing from pyspark\n",
    "    # $example on$\n",
    "    # Read in the ratings data\n",
    "    lines = sc.textFile(\"data/mllib/sample_movielens_data.txt\")\n",
    "\n",
    "    def parseLine(line):\n",
    "        fields = line.split(\"::\")\n",
    "        return Rating(int(fields[0]), int(fields[1]), float(fields[2]) - 2.5)\n",
    "    ratings = lines.map(lambda r: parseLine(r))\n",
    "\n",
    "    # Train a model on to predict user-product ratings\n",
    "    model = ALS.train(ratings, 10, 10, 0.01)\n",
    "\n",
    "    # Get predicted ratings on all existing user-product pairs\n",
    "    testData = ratings.map(lambda p: (p.user, p.product))\n",
    "    predictions = model.predictAll(testData).map(lambda r: ((r.user, r.product), r.rating))\n",
    "\n",
    "    ratingsTuple = ratings.map(lambda r: ((r.user, r.product), r.rating))\n",
    "    scoreAndLabels = predictions.join(ratingsTuple).map(lambda tup: tup[1])\n",
    "\n",
    "    # Instantiate regression metrics to compare predicted and actual ratings\n",
    "    metrics = RegressionMetrics(scoreAndLabels)\n",
    "\n",
    "    # Root mean squared error\n",
    "    print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "    # R-squared\n",
    "    print(\"R-squared = %s\" % metrics.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# $example off$\n",
    "\n",
    "\n",
    "#sc = SparkContext(appName=\"StandardScalerExample\")  # SparkContext\n",
    "\n",
    "# $example on$\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)\n",
    "\n",
    "scaler1 = StandardScaler().fit(features)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "\n",
    "# data1 will be unit variance.\n",
    "data1 = label.zip(scaler1.transform(features))\n",
    "\n",
    "# data2 will be unit variance and zero mean.\n",
    "data2 = label.zip(scaler2.transform(features.map(lambda x: Vectors.dense(x.toArray()))))\n",
    "# $example off$\n",
    "\n",
    "print(\"data1:\")\n",
    "for each in data1.collect():\n",
    "    print(each)\n",
    "\n",
    "print(\"data2:\")\n",
    "for each in data2.collect():\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Importthe library for kmeans\n",
    "from pyspark.mllib.clustering import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collborate Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=610, product=81132, rating=2.4023209296037655),\n",
       " Rating(user=365, product=155820, rating=0.3328987434190438)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile('./ratings.csv')\n",
    "# column name\n",
    "header = data.first()\n",
    "#filter header(column name), since it should not be into calculation\n",
    "ratings = data.filter(lambda row: row!=header).map(lambda x: x.split(','))\n",
    "#select useful features and filter ratings(y)\n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]),float(line[2])))\n",
    "#split data into training set and test set\n",
    "training_data, test_data = ratings_final.randomSplit([0.8,0.2])\n",
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the model for the test data = 1.24\n"
     ]
    }
   ],
   "source": [
    "rates = ratings_final.map(lambda r: ((r[0],r[1]),r[2]))\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0],r[1]),r[2]))\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "#Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Several of the methods available in scala are currently missing from pyspark\n",
    "# Load training data in LIBSVM format\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_binary_classification_data.txt\")\n",
    "\n",
    "# Split data into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "# $example off$\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bisection K-means cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# $example on$\n",
    "from numpy import array\n",
    "# $example off$\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.clustering import BisectingKMeans\n",
    "# $example off$\n",
    "\n",
    "# $example on$\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "model = BisectingKMeans.train(parsedData, 2, maxIterations=5)\n",
    "\n",
    "# Evaluate clustering\n",
    "cost = model.computeCost(parsedData)\n",
    "print(\"Bisecting K-means Cost = \" + str(cost))\n",
    "# $example off$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split(' ')])\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print(\"Usage: kmeans <file> <k>\", file=sys.stderr)\n",
    "    sys.exit(-1)\n",
    "sc = SparkContext(appName=\"KMeans\")\n",
    "lines = sc.textFile(sys.argv[1])\n",
    "data = lines.map(parseVector)\n",
    "k = int(sys.argv[2])\n",
    "model = KMeans.train(data, k)\n",
    "print(\"Final centers: \" + str(model.clusterCenters))\n",
    "print(\"Total Cost: \" + str(model.computeCost(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "# $example on$\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                    impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n",
    "sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeRegressionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n",
    "sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n",
    "# $example off$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# $example off$\n",
    "\n",
    "sc = SparkContext(appName=\"PythonGradientBoostedTreesClassificationExample\")\n",
    "# $example on$\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GradientBoostedTrees model.\n",
    "#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#         (b) Use more iterations in practice.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification GBT model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myGradientBoostingClassificationModel\")\n",
    "sameModel = GradientBoostedTreesModel.load(sc,\n",
    "                                           \"target/tmp/myGradientBoostingClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# $example off$\n",
    "\n",
    "sc = SparkContext(appName=\"PythonGradientBoostedTreesClassificationExample\")\n",
    "# $example on$\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GradientBoostedTrees model.\n",
    "#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#         (b) Use more iterations in practice.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification GBT model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myGradientBoostingClassificationModel\")\n",
    "sameModel = GradientBoostedTreesModel.load(sc,\n",
    "                                           \"target/tmp/myGradientBoostingClassificationModel\")\n",
    "# $example off$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# $example off$\n",
    "\n",
    "sc = SparkContext(appName=\"PythonRandomForestClassificationExample\")\n",
    "# $example on$\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myRandomForestClassificationModel\")\n",
    "sameModel = RandomForestModel.load(sc, \"target/tmp/myRandomForestClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# $example off$\n",
    "\n",
    "sc = SparkContext(appName=\"PythonRandomForestRegressionExample\")\n",
    "# $example on$\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                    numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                    impurity='variance', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myRandomForestRegressionModel\")\n",
    "sameModel = RandomForestModel.load(sc, \"target/tmp/myRandomForestRegressionModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
