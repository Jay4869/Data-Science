{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_month(month):\n",
    "    \"\"\"\n",
    "    Converts a string from the format M in datetime format.\n",
    "    Example: parse_month(\"2007M02\") returns datetime(2007, 2, 1)\n",
    "    \"\"\"\n",
    "    return pd.datetime(int(month[:4]), int(month[-2:]), 1)\n",
    "\n",
    "temperature = pd.read_csv('TempUSA.csv', parse_dates=['Date'], \n",
    "                          date_parser=parse_month, \n",
    "                          index_col=['Date'], # will become an index\n",
    "                          # use a subset of the columns\n",
    "                          usecols=['Date', \n",
    "                                   'LosAngelesMax', 'LosAngelesMin'])\n",
    "print temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation = pd.read_csv('./National.csv')\n",
    "nation.head()\n",
    "qs = occupancy['Quarter'].str.split('Q').apply(lambda x: '-Q'.join(x[::-1]))\n",
    "occupancy['date'] = pd.to_datetime(qs)\n",
    "occupancy = occupancy.set_index('date')\n",
    "occupancy['Average Occupancy'] = occupancy['Average Occupancy'].str.replace('%','').astype(float)\n",
    "occupancy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "occupancy['Average Occupancy'].plot(figsize = (20,6), label = 'Average Occupancy')\n",
    "occupancy['Median Occupancy'].plot(figsize = (20,6),label = 'Median Occupancy')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Occupancy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,8\n",
    "y = occupancy['Average Occupancy']\n",
    "decomposition = sm.tsa.seasonal_decompose(y, model = 'additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0,2)\n",
    "pdq = list(itertools.product(p,d,q))\n",
    "\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "            a.append(results.aic)\n",
    "        except:\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=(2, 0, 1),\n",
    "                                seasonal_order=(1, 1, 2, 4),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary().tables[1])\n",
    "results.plot_diagnostics(figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dynamic occupancy test\n",
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "ax = y['2005':].plot(label='observed', figsize=(10, 4))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2017-01-01'), y.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('occupancy(%)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_forecasted = pred_dynamic.predicted_mean\n",
    "y_truth = y['1998-01-01':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecast 500 steps ahead in future\n",
    "pred_uc = results.get_forecast(steps=8)\n",
    "\n",
    "# Get confidence intervals of forecasts\n",
    "pred_ci = pred_uc.conf_int()\n",
    "ax = y.plot(label='observed', figsize=(10, 4))\n",
    "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Occupancy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "pred_uc.predicted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge,BayesianRidge #For lasso and ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Value \n",
    "# refill visib using forward or backward\n",
    "data['visib'] = data['visib'].replace(999.9, np.nan).fillna(method='ffill')\n",
    "data[\"Gender\"].fillna(\"No Gender\", inplace = True)  \n",
    "data.replace(to_replace = np.nan, value = -99)  \n",
    "df.dropna(axis = 1) \n",
    "# previous ones   \n",
    "df.fillna(method ='pad') \n",
    "# using notnull() function  \n",
    "df.notnull() \n",
    "df.isnull()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=10)\n",
    "\n",
    "### Scale the data\n",
    "from sklearn import preprocessing\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)\n",
    "y_train_scaled = y_train - np.mean(y_train)\n",
    "y_test_scaled = y_test - np.mean(y_test)\n",
    "\n",
    "selected_features = ['month', 'pickup_hour', 'weekday', 'holiday','rush_hour',\n",
    "                     'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "                     'temp','visib', 'mxpsd',  'prcp', 'sndp', 'fog', 'passenger_count',\n",
    "                     'bearing', 'airport','manhattan_distance','haversine_distance', 'dropoff_geo_recode'\n",
    "                    ]\n",
    "\n",
    "x = data[selected_features]\n",
    "y = data['log_travel_time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model evaluance\n",
    "def train_test_model_performance(clf, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test):\n",
    "    \n",
    "    # Fit a model by providing X and y from training set\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(clf)\n",
    "    # Make prediction on the training data\n",
    "    y_train_pred = clf.predict(x_train)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    y_test_pred = clf.predict(x_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    R2_Train = clf.score(x_train,y_train)\n",
    "    R2_Test = clf.score(x_test,y_test)\n",
    "    RMSE_Train = sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    RMSE_Test = sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     RMSLE_Train = sqrt(mean_squared_log_error(y_train, y_train_pred))\n",
    "#     RMSLE_Test = sqrt(mean_squared_log_error(y_test, y_test_pred))\n",
    "    \n",
    "    metric_names = ['R2','RMSE']\n",
    "    metric_values_train = [R2_Train, RMSE_Train]\n",
    "    metric_values_test = [R2_Test, RMSE_Test]\n",
    "    all_metrics = pd.DataFrame({'metrics':metric_names,\n",
    "                                'train':metric_values_train,\n",
    "                                'test':metric_values_test},columns=['metrics','train','test']).set_index('metrics')\n",
    "    print(all_metrics)\n",
    "\n",
    "def cv_model_performance(clf, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test):\n",
    "    \n",
    "    # Fit a model by providing X and y from training set\n",
    "    clf.fit(x_train, y_train)\n",
    "    model = clf.best_estimator_\n",
    "    print(model)\n",
    "    \n",
    "    # Make prediction on the training data\n",
    "    y_train_pred = model.predict(x_train)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    R2_Train = model.score(x_train,y_train)\n",
    "    R2_Test = model.score(x_test,y_test)\n",
    "    RMSE_Train = sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    RMSE_Test = sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    RMSLE_Train = sqrt(mean_squared_log_error(y_train, y_train_pred))\n",
    "    RMSLE_Test = sqrt(mean_squared_log_error(y_test, y_test_pred))\n",
    "    \n",
    "    metric_names = ['R2','RMSE','RMSLE']\n",
    "    metric_values_train = [R2_Train, RMSE_Train, RMSLE_Train]\n",
    "    metric_values_test = [R2_Test, RMSE_Test, RMSLE_Test]\n",
    "    all_metrics = pd.DataFrame({'metrics':metric_names,\n",
    "                                'train':metric_values_train,\n",
    "                                'test':metric_values_test},columns=['metrics','train','test']).set_index('metrics')\n",
    "    print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# parameters = {'n_estimators': 300,\n",
    "#              'max_features': 'auto',\n",
    "#               'max_depth': 50,\n",
    "#               'min_samples_split': 40,\n",
    "#               'random_state': 2,\n",
    "#               'n_jobs': -1,\n",
    "#               }\n",
    "# 0.899009  0.839094 25\n",
    "\n",
    "parameters = {'n_estimators': 300,\n",
    "             'max_features': 'auto',\n",
    "              'max_depth': 70,\n",
    "              'min_samples_split': 150,\n",
    "              'random_state': 2,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "\n",
    "rf = RandomForestRegressor(**parameters)\n",
    "train_test_model_performance(rf)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken by above cell is {}.\".format(round(end-start)/60,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(list(zip(selected_features, rf.feature_importances_))).sort_values(by=[1], ascending=False)\n",
    "features.columns = ['feature', 'value']\n",
    "\n",
    "ax = features.iloc[0:10].plot.barh()\n",
    "t = np.arange(11)\n",
    "ax.set_yticks(t)\n",
    "ax.set_yticklabels(features['feature'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression().fit(x, y)\n",
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import packages\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2a: Provide data\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "# Step 2b: Transform input data\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "r_sq = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_\n",
    "\n",
    "# Step 5: Predict\n",
    "y_pred = model.predict(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_square = []\n",
    "coefs =[]\n",
    "#loop and calculate r square for each alpha\n",
    "for i in range(len(alphas)):\n",
    "    clf = linear_model.Lasso(alpha = alphas[i])\n",
    "    clf.fit(X_train_scaled,y_train_scaled)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    r_square.append(clf.score(X_test_scaled,y_test_scaled))\n",
    "    coefs.append(clf.coef_)\n",
    "#Plot\n",
    "coef = clf.coef_\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(alphas,r_square)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"r_square\")\n",
    "ax.grid(b=True, which='major', color='k', linestyle='--')\n",
    "\n",
    "print(\"So the best alpha is {0:.3f}. The new score is {1:.3f}.\".format(alphas[np.argmax(r_square)], np.max(r_square)))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Student's t-test\n",
    "from scipy.stats import ttest_ind\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_ind(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Paired Student's t-test\n",
    "from scipy.stats import ttest_rel\n",
    "stat, p = ttest_rel(data1, data2)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Analysis of Variance Test\n",
    "from scipy.stats import f_oneway\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "data3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\n",
    "stat, p = f_oneway(data1, data2, data3)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Chi-Squared Test\n",
    "from scipy.stats import chi2_contingency\n",
    "table = [[10, 20, 30],[6,  9,  17]]\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "# Code here\n",
    "X_train.describe()\n",
    "intercept=np.ones(df.shape[0])\n",
    "X.insert(0,\"intercept\",intercept)\n",
    "beta = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "y_pred1 = X_test.dot(beta)\n",
    "y_pred1.head()\n",
    "beta \n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / folds)\n",
    "\tfor i in range(folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# test cross validation split\n",
    "\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "folds = cross_validation_split(dataset, 4)\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
