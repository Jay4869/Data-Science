{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "#url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "sc =SparkContext()\n",
    "#sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.csv(\"./adult_data.csv\", header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|  x|age|workclass|fnlwgt|   education|educational-num|    marital-status|       occupation|relationship| race|gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|  1| 25|  Private|226802|        11th|              7|     Never-married|Machine-op-inspct|   Own-child|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "|  2| 38|  Private| 89814|     HS-grad|              9|Married-civ-spouse|  Farming-fishing|     Husband|White|  Male|           0|           0|            50| United-States| <=50K|\n",
      "|  3| 28|Local-gov|336951|  Assoc-acdm|             12|Married-civ-spouse|  Protective-serv|     Husband|White|  Male|           0|           0|            40| United-States|  >50K|\n",
      "|  4| 44|  Private|160323|Some-college|             10|Married-civ-spouse|Machine-op-inspct|     Husband|Black|  Male|        7688|           0|            40| United-States|  >50K|\n",
      "|  5| 18|        ?|103497|Some-college|             10|     Never-married|                ?|   Own-child|White|Female|           0|           0|            30| United-States| <=50K|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: float (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: float (nullable = true)\n",
      " |-- capital-loss: float (nullable = true)\n",
      " |-- hours-per-week: float (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital-gain', 'educational-num', 'capital-loss', 'hours-per-week']\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|  fnlwgt|\n",
      "+----+--------+\n",
      "|25.0|226802.0|\n",
      "|38.0| 89814.0|\n",
      "|28.0|336951.0|\n",
      "|44.0|160323.0|\n",
      "|18.0|103497.0|\n",
      "+----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "#stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "#model = stringIndexer.fit(df)\n",
    "#df = model.transform(df)\n",
    "df.select('age','fnlwgt').show(5)\n",
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital-gain|\n",
      "+-------+------------------+\n",
      "|  count|             48842|\n",
      "|   mean|1079.0676262233324|\n",
      "| stddev| 7452.019057655418|\n",
      "|    min|               0.0|\n",
      "|    max|           99999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('capital-gain').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|age_income|<=50K|>50K|\n",
      "+----------+-----+----+\n",
      "|      17.0|  595|   0|\n",
      "|      18.0|  862|   0|\n",
      "|      19.0| 1050|   3|\n",
      "|      20.0| 1112|   1|\n",
      "|      21.0| 1090|   6|\n",
      "|      22.0| 1161|  17|\n",
      "|      23.0| 1307|  22|\n",
      "|      24.0| 1162|  44|\n",
      "|      25.0| 1119|  76|\n",
      "|      26.0| 1068|  85|\n",
      "|      27.0| 1117| 115|\n",
      "|      28.0| 1101| 179|\n",
      "|      29.0| 1025| 198|\n",
      "|      30.0| 1031| 247|\n",
      "|      31.0| 1050| 275|\n",
      "|      32.0|  957| 296|\n",
      "|      33.0| 1045| 290|\n",
      "|      34.0|  949| 354|\n",
      "|      35.0|  997| 340|\n",
      "|      36.0|  948| 400|\n",
      "+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In some occasion, it can be interesting to see the descriptive statistics between two pairwise columns. \n",
    "#For instance, you can count the number of people with income below or above 50k by education level. \n",
    "#This operation is called a crosstab.\n",
    "df.crosstab('age', 'income').sort(\"age_income\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|      marital-status| avg(capital-gain)|\n",
      "+--------------------+------------------+\n",
      "|           Separated| 581.8424836601307|\n",
      "|       Never-married|  384.382639449029|\n",
      "|Married-spouse-ab...| 629.0047770700637|\n",
      "|            Divorced| 793.6755615860094|\n",
      "|             Widowed| 603.6442687747035|\n",
      "|   Married-AF-spouse|2971.6216216216217|\n",
      "|  Married-civ-spouse|1739.7006121810625|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "There are two intuitive API to drop columns:\n",
    "\n",
    "drop(): Drop a column\n",
    "dropna(): Drop NA's\n",
    "Below you drop the column education_num\n",
    "\n",
    "'''\n",
    "df.drop('education_num').columns\n",
    "df.filter(df.age > 40).count()\n",
    "df.groupby(['marital-status']).agg({'capital-gain': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing is a critical step in machine learning. After you remove garbage data, you get some important insights. For instance, you know that age is not a linear function with the income. When people are young, their income is usually lower than mid-age. After retirement, a household uses their saving, meaning a decrease in income. To capture this pattern, you can add a square to the age feature\n",
    "\n",
    "Add age square\n",
    "\n",
    "To add a new feature, you need to:\n",
    "\n",
    "Select the column\n",
    "Apply the transformation and add it to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: float (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: float (nullable = true)\n",
      " |-- capital-loss: float (nullable = true)\n",
      " |-- hours-per-week: float (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1 Select the column\n",
    "age_square = df.select(col(\"age\")**2)\n",
    "\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age_square\", col(\"age\")**2)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=25.0, age_square=625.0, workclass='Private', fnlwgt=226802.0, education='11th', educational-num=7.0, marital-status='Never-married', occupation='Machine-op-inspct', relationship='Own-child', race='Black', gender='Male', capital-gain=0.0, capital-loss=0.0, hours-per-week=40.0, native-country='United-States', income='<=50K')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = ['age', 'age_square', 'workclass', 'fnlwgt', 'education', 'educational-num', 'marital-status',\n",
    "           'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss',\n",
    "           'hours-per-week', 'native-country', 'income']\n",
    "df = df.select(COLUMNS)\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude Holand-Netherlands\n",
    "\n",
    "When a group within a feature has only one observation, it brings no information to the model. On the contrary, it can lead to an error during the cross-validation.\n",
    "\n",
    "Let's check the origin of the household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native-country|count(native-country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|             Hungary|                   19|\n",
      "|            Honduras|                   20|\n",
      "|            Scotland|                   21|\n",
      "|Outlying-US(Guam-...|                   23|\n",
      "|          Yugoslavia|                   23|\n",
      "|                Laos|                   23|\n",
      "|     Trinadad&Tobago|                   27|\n",
      "|            Cambodia|                   28|\n",
      "|                Hong|                   30|\n",
      "|            Thailand|                   30|\n",
      "|             Ireland|                   37|\n",
      "|              France|                   38|\n",
      "|             Ecuador|                   45|\n",
      "|                Peru|                   46|\n",
      "|              Greece|                   49|\n",
      "|           Nicaragua|                   49|\n",
      "|                Iran|                   59|\n",
      "|              Taiwan|                   65|\n",
      "|            Portugal|                   67|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['native-country'] == 'Holand-Netherlands').count()\n",
    "df.groupby('native-country').agg({'native-country': 'count'}).sort(asc(\"count(native-country)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df.filter(df['native-country'] != 'Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a data preprocessing Pipeline\n",
    "\n",
    "Similar to scikit-learn, Pyspark has a pipeline API. A pipeline is very convenient to maintain the structure of the data. You push the data into the pipeline. Inside the pipeline, various operations are done, the output is used to feed the algorithm.\n",
    "\n",
    "For instance, one universal transformation in machine learning consists of converting a string to one hot encoder, i.e., one column by a group. One hot encoder is usually a matrix full of zeroes.\n",
    "\n",
    "The steps to transform the data are very similar to scikit-learn. You need to:\n",
    "\n",
    "Index the string to numeric\n",
    "\n",
    "Create the one hot encoder\n",
    "\n",
    "Transform the data\n",
    "\n",
    "Two APIs do the job: StringIndexer, OneHotEncoder\n",
    "\n",
    "First of all, you select the string column to index. The inputCol is the name of the column in the dataset. outputCol is the new name given to the transformed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------------+--------+------------+---------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+-----------------+-------------+\n",
      "| age|age_square|       workclass|  fnlwgt|   education|educational-num|    marital-status|       occupation| relationship| race|gender|capital-gain|capital-loss|hours-per-week|native-country|income|workclass_encoded|workclass_vec|\n",
      "+----+----------+----------------+--------+------------+---------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+-----------------+-------------+\n",
      "|25.0|     625.0|         Private|226802.0|        11th|            7.0|     Never-married|Machine-op-inspct|    Own-child|Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|              0.0|(9,[0],[1.0])|\n",
      "|38.0|    1444.0|         Private| 89814.0|     HS-grad|            9.0|Married-civ-spouse|  Farming-fishing|      Husband|White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|              0.0|(9,[0],[1.0])|\n",
      "|28.0|     784.0|       Local-gov|336951.0|  Assoc-acdm|           12.0|Married-civ-spouse|  Protective-serv|      Husband|White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|              2.0|(9,[2],[1.0])|\n",
      "|44.0|    1936.0|         Private|160323.0|Some-college|           10.0|Married-civ-spouse|Machine-op-inspct|      Husband|Black|  Male|      7688.0|         0.0|          40.0| United-States|  >50K|              0.0|(9,[0],[1.0])|\n",
      "|18.0|     324.0|               ?|103497.0|Some-college|           10.0|     Never-married|                ?|    Own-child|White|Female|         0.0|         0.0|          30.0| United-States| <=50K|              3.0|(9,[3],[1.0])|\n",
      "|34.0|    1156.0|         Private|198693.0|        10th|            6.0|     Never-married|    Other-service|Not-in-family|White|  Male|         0.0|         0.0|          30.0| United-States| <=50K|              0.0|(9,[0],[1.0])|\n",
      "|29.0|     841.0|               ?|227026.0|     HS-grad|            9.0|     Never-married|                ?|    Unmarried|Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|              3.0|(9,[3],[1.0])|\n",
      "|63.0|    3969.0|Self-emp-not-inc|104626.0| Prof-school|           15.0|Married-civ-spouse|   Prof-specialty|      Husband|White|  Male|      3103.0|         0.0|          32.0| United-States|  >50K|              1.0|(9,[1],[1.0])|\n",
      "|24.0|     576.0|         Private|369667.0|Some-college|           10.0|     Never-married|    Other-service|    Unmarried|White|Female|         0.0|         0.0|          40.0| United-States| <=50K|              0.0|(9,[0],[1.0])|\n",
      "|55.0|    3025.0|         Private|104996.0|     7th-8th|            4.0|Married-civ-spouse|     Craft-repair|      Husband|White|  Male|         0.0|         0.0|          10.0| United-States| <=50K|              0.0|(9,[0],[1.0])|\n",
      "+----+----------+----------------+--------+------------+---------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "#First of all, you select the string column to index. The inputCol is the name of the column in the dataset. \n",
    "#outputCol is the new name given to the transformed column.\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "#Fit the data and transform it\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "#Create the news columns based on the group. For instance, if there are 10 groups in the feature, \n",
    "#the new matrix will have 10 columns, one for each group.\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "You will build a pipeline to convert all the precise features and add them to the final dataset. The pipeline will have four operations, but feel free to add as many operations as you want.\n",
    "\n",
    "Encode the categorical data\n",
    "Index the label feature\n",
    "Add continuous variable\n",
    "Assemble the steps.\n",
    "Each step is stored in a list named stages. This list will tell the VectorAssembler what operation to perform inside the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#This step is exaclty the same as the above example, except that you loop over all the categorical features.\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(dropLast=False, inputCol= str(stringIndexer.getOutputCol()),\n",
    "                                     outputCol= str(categoricalCol + \"classVec\"))\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Convert label into label indices using the StringIndexer\n",
    "income_stringIdx =  StringIndexer(inputCol=\"income\", outputCol=\"newincome\")\n",
    "stages += [income_stringIdx]\n",
    "#The inputCols of the VectorAssembler is a list of columns. You can create a new list containing all the new columns.\n",
    "#The code below popluate the list with encoded categorical features and the continuous features.\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES\n",
    "#Finally, you pass all the steps in the VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_4d2fa2592e1fc0948b18,\n",
       " OneHotEncoder_431d8bf57def74b4815f,\n",
       " StringIndexer_4164b79116559c5ba62b,\n",
       " OneHotEncoder_418e9c323747940ae1a6,\n",
       " StringIndexer_48ecb3d82fce4b3b8b05,\n",
       " OneHotEncoder_4a3abf4aea05a2e31cc6,\n",
       " StringIndexer_445aa3a92794c9b22b7a,\n",
       " OneHotEncoder_40cb8018cf1ecc237c9f,\n",
       " StringIndexer_4032922e0b459e691e56,\n",
       " OneHotEncoder_49bea3884f914ac44d71,\n",
       " StringIndexer_439aa32ee3d862348e1b,\n",
       " OneHotEncoder_4d6f8978799bbb44fc72,\n",
       " StringIndexer_4faf812980c7d31a830c,\n",
       " OneHotEncoder_4d39a45f71db3d501d7a,\n",
       " StringIndexer_490194f68c0a9851c1a3,\n",
       " OneHotEncoder_48ff84a8d360530ca9de,\n",
       " StringIndexer_403ea03214d5176d73c7,\n",
       " VectorAssembler_45e3918b342d34cb6816]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=25.0, age_square=625.0, workclass='Private', fnlwgt=226802.0, education='11th', educational-num=7.0, marital-status='Never-married', occupation='Machine-op-inspct', relationship='Own-child', race='Black', gender='Male', capital-gain=0.0, capital-loss=0.0, hours-per-week=40.0, native-country='United-States', income='<=50K', workclassIndex=0.0, workclassclassVec=SparseVector(9, {0: 1.0}), educationIndex=5.0, educationclassVec=SparseVector(16, {5: 1.0}), marital-statusIndex=1.0, marital-statusclassVec=SparseVector(7, {1: 1.0}), occupationIndex=6.0, occupationclassVec=SparseVector(15, {6: 1.0}), relationshipIndex=2.0, relationshipclassVec=SparseVector(6, {2: 1.0}), raceIndex=1.0, raceclassVec=SparseVector(5, {1: 1.0}), genderIndex=0.0, genderclassVec=SparseVector(2, {0: 1.0}), native-countryIndex=0.0, native-countryclassVec=SparseVector(41, {0: 1.0}), newincome=0.0, features=SparseVector(107, {0: 1.0, 14: 1.0, 26: 1.0, 38: 1.0, 49: 1.0, 54: 1.0, 58: 1.0, 60: 1.0, 101: 25.0, 102: 226802.0, 104: 7.0, 106: 40.0}))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)\n",
    "model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|income|            features|\n",
      "+------+--------------------+\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   1.0|[0.0,0.0,1.0,0.0,...|\n",
      "|   1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[0.0,0.0,0.0,1.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[0.0,0.0,0.0,1.0,...|\n",
      "|   1.0|[0.0,1.0,0.0,0.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[0.0,0.0,0.0,1.0,...|\n",
      "|   1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|   1.0|[1.0,0.0,0.0,0.0,...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newincome\"], DenseVector(x[\"features\"])))\n",
    "df_train = sqlContext.createDataFrame(input_data, [\"income\", \"features\"])\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|income|count(income)|\n",
      "+------+-------------+\n",
      "|   0.0|        29675|\n",
      "|   1.0|         9300|\n",
      "+------+-------------+\n",
      "\n",
      "+------+-------------+\n",
      "|income|count(income)|\n",
      "+------+-------------+\n",
      "|   0.0|         7479|\n",
      "|   1.0|         2387|\n",
      "+------+-------------+\n",
      "\n",
      "Coefficients: [-0.09011608130644566,-0.1384380559306834,-0.03896303543531769,-0.1824457748448405,-0.11893792188013645,0.18847080499878685,0.1839760119625868,-0.24212542028408682,-0.2943767644727913,-0.19865062889469637,-0.07602389181616244,0.21440405239433685,0.3825828010868464,-0.01510053172331298,-0.29743026128624805,-0.022118183258364482,-0.31327611253729226,-0.3943831805361721,0.5329648681065523,-0.3451003029762354,-0.2100999415985072,0.5880134963170344,-0.3270540179267765,-0.37208279515977744,-0.32857180162206834,0.32803921613790427,-0.35597749175340104,-0.20951536020969735,-0.21172515919880228,-0.14186610091919277,-0.13546494888365676,0.12502584341953393,0.2038127961969423,-0.07783296791130631,0.3061983471227641,-0.11111455404510757,0.0343460030126342,-0.29474222854335225,-0.21392266275725846,-0.1828256507695897,-0.13295599940610825,-0.28862265341572235,-0.3216764586476885,0.10612982657479653,0.08456143961857533,-0.2487395001577925,0.0312033980558303,0.26380867641720374,-0.22225756185854295,-0.32474548735785086,-0.2483545091601607,0.40893828713533253,-0.26261774057780585,-0.09576435219923389,-0.17982386591323127,-0.08688411733336236,-0.2526557676058329,-0.19139606130469006,0.06530972124828313,-0.21963606721681042,-0.15746757651625143,-0.3446225146006371,-0.16678631079208292,-0.0435041296228696,-0.06753023360999215,-0.2097684676290631,-0.0426520722205701,-0.25563414004237084,-0.1172847642900367,-0.16206958830682647,0.11948039396622082,-0.21806087738775698,-0.3493088456628903,-0.13178168084299574,0.09932249861414028,-0.36396694909500393,-0.0863445254833131,-0.26059945401934215,-0.13655200898078013,-0.37328192781832986,-0.5305878374231666,-0.17802940620675514,-0.10492574151397246,-0.12979649431009335,0.0053499565562030215,-0.013430750828135777,-0.32105170556023266,-0.39060815148153866,-0.2753001160498495,0.17799334989826157,0.052385549272066345,-0.3925537567903844,-0.26086609058296334,0.1539496051104925,-0.3743793405031432,0.20000694351301052,-0.4673518607822372,-0.3810740110886731,-0.3143731220871077,-0.18229091599931382,0.1103718689297456,0.004078525320230339,-1.1769914851624496e-07,2.116143793007515e-05,0.021018102857023175,0.00021800033004563913,0.005387739702646929]\n",
      "Intercept: -1.3995870768877647\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)\n",
    "train_data.groupby('income').agg({'income': 'count'}).show()\n",
    "test_data.groupby('income').agg({'income': 'count'}).show()\n",
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"income\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- income: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+------+----------+--------------------+\n",
      "|income|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|   0.0|       0.0|[0.92734536207514...|\n",
      "|   0.0|       0.0|[0.93874864367220...|\n",
      "|   0.0|       0.0|[0.93510980388595...|\n",
      "|   0.0|       0.0|[0.91743304613848...|\n",
      "|   0.0|       0.0|[0.61977331542055...|\n",
      "|   0.0|       0.0|[0.64604312338942...|\n",
      "|   0.0|       0.0|[0.65246551845643...|\n",
      "|   0.0|       0.0|[0.72446047629837...|\n",
      "|   0.0|       0.0|[0.91469778328490...|\n",
      "|   0.0|       0.0|[0.84471647810027...|\n",
      "|   0.0|       0.0|[0.83272583367082...|\n",
      "|   0.0|       0.0|[0.83454419741497...|\n",
      "|   0.0|       0.0|[0.52944496204343...|\n",
      "|   0.0|       0.0|[0.63524895812302...|\n",
      "|   0.0|       0.0|[0.63030940685612...|\n",
      "|   0.0|       0.0|[0.56876018042796...|\n",
      "|   0.0|       0.0|[0.87741605592912...|\n",
      "|   0.0|       0.0|[0.79930348588550...|\n",
      "|   0.0|       0.0|[0.84448631889194...|\n",
      "|   0.0|       0.0|[0.51648567840431...|\n",
      "|   0.0|       0.0|[0.58138412943373...|\n",
      "|   0.0|       0.0|[0.58603380841517...|\n",
      "|   0.0|       0.0|[0.61693767460693...|\n",
      "|   0.0|       0.0|[0.73936296956482...|\n",
      "|   0.0|       1.0|[0.33573015493965...|\n",
      "|   0.0|       0.0|[0.74936838053515...|\n",
      "|   0.0|       0.0|[0.75059744130359...|\n",
      "|   0.0|       0.0|[0.80061011024775...|\n",
      "|   0.0|       0.0|[0.76640228351472...|\n",
      "|   0.0|       0.0|[0.75600437173396...|\n",
      "|   0.0|       0.0|[0.68247028951732...|\n",
      "|   0.0|       0.0|[0.76711918311576...|\n",
      "|   0.0|       0.0|[0.66562175737636...|\n",
      "|   0.0|       0.0|[0.72373987352819...|\n",
      "|   0.0|       0.0|[0.76078468641368...|\n",
      "|   0.0|       0.0|[0.75464214185619...|\n",
      "|   0.0|       0.0|[0.71033167596332...|\n",
      "|   0.0|       0.0|[0.72576269754493...|\n",
      "|   0.0|       0.0|[0.77704512912713...|\n",
      "|   0.0|       0.0|[0.72959609770746...|\n",
      "|   0.0|       1.0|[0.44958628377571...|\n",
      "|   0.0|       1.0|[0.45438949920956...|\n",
      "|   0.0|       1.0|[0.41591431193724...|\n",
      "|   0.0|       1.0|[0.45222112265924...|\n",
      "|   0.0|       1.0|[0.43989266416744...|\n",
      "|   0.0|       1.0|[0.44017777856935...|\n",
      "|   0.0|       0.0|[0.83819940941958...|\n",
      "|   0.0|       0.0|[0.82898910236888...|\n",
      "|   0.0|       0.0|[0.81123962137545...|\n",
      "|   0.0|       0.0|[0.77184644926278...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "+------+-------------+\n",
      "|income|count(income)|\n",
      "+------+-------------+\n",
      "|   0.0|         7479|\n",
      "|   1.0|         2387|\n",
      "+------+-------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|             8878|\n",
      "|       1.0|              988|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82090006081492"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)\n",
    "predictions.printSchema()\n",
    "selected = predictions.select(\"income\", \"prediction\", \"probability\")\n",
    "selected.show(50)\n",
    "cm = predictions.select(\"income\", \"prediction\")\n",
    "cm.groupby('income').agg({'income': 'count'}).show()\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()\n",
    "cm.filter(cm.income == cm.prediction).count() / cm.count()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 82.090%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"income\", \"prediction\")\n",
    "    acc = cm.filter(cm.income == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)\n",
    "#Model accuracy: 82.376%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(cm))\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())\n",
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)\n",
    "accuracy_m(model = cvModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
