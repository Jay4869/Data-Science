---
title: "Boston Housing prediction"
author: "Jie Li"
date: "December 25, 2018"
output:
  html_document: default
  pdf_document: default
fontsize: 14pt
geometry: margin=1in
---

### Abstract
The goal of this analysis is to fit a regression model that best explains the variation in Boston house price. Various statistical techniques were used to eliminate predictors and extraneous.

The first thing that can be interpreted is that the average number of rooms is positively correlated with house price. Second, there is also a positive correlation if the house is next to the Charles River. It is reasonable that more people like to live closer to the river for the great view on offer and that this should raise the house prices. Similarly, negative correlations with lower status of the population and pupil-teacher ratios are also to be expected. People would prefer to live in areas that have a lower pupil-teacher ratio and high status of population.

Most importantly, higher levels of pollution decrease house prices. People would prefer to live further away from high nitric oxides area. In the years since, there is no doubt that pollution levels have risen and it would be interesting to examine the ways in which that affects house pricing in Boston today.

### Boston Housing Dataset
The Boston housing data to be analyzed were collected by Harrison and Rubinfeld in 1978, and contains 506 census tracts of Boston from the 1970 census. The purpose of discovering whether or not clean air influenced the value of houses in Boston.

The data `BostonHousing2` is the corrected version with additional spatial information. Therefore, I am using `BostonHousing2` dataset from `mlbench` library. The following is a brief description of each feature and the outcome in our dataset:

* cmedv - target variable, correctly version median value of owner-occupied homes in USD 1000's
* crim - per capita crime rate by town
* zn - proportion of residential land zoned for lots over 25,000 sq.ft
* indus - proportion of non-retail business acres per town
* chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
* nox - nitric oxides concentration (parts per 10 million)
* rm - average number of rooms per dwelling
* age - proportion of owner-occupied units built prior to 1940
* dis - weighted distances to five Boston employment centres
* rad - index of accessibility to radial highways
* tax - full-value property-tax rate per USD 10,000
* ptratio- pupil-teacher ratio by town
* b - 1000(Black - 0.63)^2, where Black is the proportion of blacks by town
* lstat - percentage of lower status of the population
* town - name of town
* tract - census tract
* lon - longitude of census tract
* lat - latitude of census tract

```{r echo=F, warning=FALSE, message=F}
# Require packages
# install.packages("mlbench")
# install.packages("corrplot")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("alr4")
# install.packages("reshape2")
# install.packages("leaps")
# install.packages("glmnet")
# install.packages("caret")

# Loading the library
library(mlbench)
library(corrplot)
library(dplyr)
library(ggplot2)
library(reshape2)
library(car)
library(alr4)
library(leaps)
library(glmnet)
library(caret)

# Load Boston Housing dataset
data(BostonHousing2)
data = BostonHousing2

# Basic statistical summary
str(data)
```

### Data Pre-processing
First of all, there is no missing data found in the Boston dataset
```{r echo=F, warning=FALSE}
# checking missing data
apply(data, 2, function(x){sum(is.na(x))})
```

### Exploratory Data Analysis
Correlation plots are a great way of exploring data and seeing any features are highly correlated with `cmedv` and any interaction terms.
```{r echo=F, warning=FALSE, fig, fig.height = 7, fig.width = 7, fig.align = "center"}
# correlation plots
corrplot(cor(select(data, -chas, -town, -medv)), type = "upper", method = "number", number.cex = 0.8)
```

`cmedv` has positive relationship with `tract`(*medium*) `zn`(*medium*), `rm`(*high*), `dis`(*low*) and `b`(*low*)

`cmedv` has negative relationship with `crim` (*medium*), `indus` (*medium*), `nox`(*medium*), `age`(*medium*), `rad`(*medium*), `tax`(*medium*), `ptratio`(*high*), `lstat`, `lon`(*low*) and `lat`(*low*)

Assume that the correlation is lower than 0.33, it means the features did not contain much information to explain `cmedv`. Therefore, I focus on the features have medium and high correlation. The following histograms are showing the distribution of selected features.
```{r echo=F, warning=FALSE, message=F, fig2, fig.height = 7, fig.width = 7, fig.align = "center"}
feature.cont = select(data, c(cmedv, crim, zn, indus, nox, rm, age, tax, rad, ptratio, lstat))

# Plot histgorm for all features
feature.cont %>%
    melt(.) %>%
    ggplot(.)+
    geom_histogram(aes(x=value, color=variable), bins = 20, fill = I("white"))+
    facet_wrap(~ variable, scales = "free", ncol = 3)+
    labs(x = "Features", y = "Counts")+
    theme_bw()+
    theme_minimal()
```

Let's see the scatter plots of `cmedv` versus each feature
```{r echo=F, warning=FALSE, fig3, fig.height = 7, fig.width = 7, fig.align = "center"}
# effect plots for each feature
feature.cont %>%
  melt(id.vars = "cmedv") %>%
  ggplot(aes(x = value, y = cmedv, color = variable))+
  geom_point()+
  geom_smooth(method = "loess", colour = "black")+
  facet_wrap(~ variable, scales = "free", ncol = 3)+
  labs(x = "Features", y = "Median House Price ($1000s)")+
  theme_bw()+
  theme_minimal()
```

The features (`cmedv`, `crim`, `indus`, `nox`, `age` and `lstat`) are not following normal distribution, and exist curvatures (non-linearly association). Also, the features (`zn`, `tax`, `tract`) are showing that the `cmedv` might be depended on two different groups. In addition, `rad` should be treated as multivariate categorical variable.

Create categorical variables to separate two groups, and use box plots and `T-test` to show the significant housing price difference between two groups.
```{r echo=F, warning=FALSE, fig4, fig.height = 7, fig.width = 7, fig.align = "center"}
# Create and boxplot a new category values
data$tax.new = factor(ifelse(data$tax >= 600, 1, 0))
data$tract.new = factor(ifelse(data$tract >= 2500, 1, 0))
data$zn.new = factor(ifelse(data$zn > 0, 1, 0))

data[, c("cmedv", "chas", "tax.new", "tract.new", "zn.new")] %>%
  melt(id.vars = "cmedv") %>%
  ggplot(.)+
  geom_boxplot(aes(value, cmedv, color = variable))+
  facet_wrap(~ variable, scales = "free", ncol = 2)+
  labs(title = "Categorical variables effects on House Price", x="", y="Median House Price")+
  theme_bw()+
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5))

# test for catergorical data
t.test(cmedv ~ chas, data)
t.test(cmedv ~ tax.new, data)
t.test(cmedv ~ tract.new, data)
t.test(cmedv ~ zn.new, data)
```

From the box-plots above, two different groups people have significantly difference on the average housing price in Boston. Also, the p-value from `T-test` supports my hypothesis. Therefore, add those categorical features into model.

### Transformation regressors & response
The purpose of transformations is to achieve a mean function that is linear in the transformed scale. The `Box-Cox` method provided a general method for selecting a transformation from a family indexed by a parameter $\lambda$. It is not transforming for linearity, but rather it is transforming for normality since $X$ is multivariate normal is much stronger than linearly related regressors.

Use `Box-Cox` power transformation on the features to make for a better fit.
```{r echo=F, warning=FALSE, fig5, fig.height = 7, fig.width = 7, fig.align = "center"}
# Transformate predictors
a = powerTransform(cbind(crim, indus, nox, rm, age, ptratio, lstat) ~ 1, feature.cont)
summary(a)
a$roundlam = c(-0.12, 0.5, -1.3, 1, 1.5, 4.5, 1/3)
names(a$roundlam) = c("crim", "indus", "nox", "rm", "age", "ptratio", "lstat")
round(a$roundlam,2)

# Testing transformation lambda
testTransform(a, lambda=a$roundlam)

# scatterplot on transformated features
feature.cont.T = data.frame(cbind(data$cmedv, data$crim^(-0.12), sqrt(data$indus), data$nox^-1.3, data$rm, data$age^1.5, data$ptratio^4.5, data$lstat^1/3))
names(feature.cont.T) = c("cmedv", "crim", "indus", "nox", "rm", "age", "ptratio", "lstat")

feature.cont.T %>%
  melt(id.vars = "cmedv") %>%
  ggplot(aes(x = value, y = cmedv, color = variable))+
  geom_point()+
  geom_smooth(method = "loess", color = "black")+
  facet_wrap(~ variable, scales = "free", ncol = 3)+
  labs(x = "Transformated Features", y = "Median House Price")+
  theme_bw()+
  theme_minimal()
```

Based on `Box-Cox` method and `likelihood ratio test (LRT)` above, there is no evidence to reject our estimated power transformation (p-value > 0.05). Then, the scatter plots of transformated features are showing that it fixs some curvatures (`crim`, `indus`, `nox` and `age`), but `rm` and `lstat` still exist curvatures, so add second-order regressors such as polynomial or interaction terms in the model.

Once the predictors are transformed, let's transform the response by inverse fitted value plot
```{r echo=F, warning=FALSE, fig6, fig.align = "center"}
# Transformate response
m1 = lm(cmedv ~ I(crim^-0.12) + sqrt(indus) + I(nox^-1.3) + rm + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3), data)

inverseResponsePlot(m1)
ggplot(data)+
  geom_histogram(aes(sqrt(cmedv)), binwidth = 0.5)+
  labs(title="Tranformed Response", x="Sqrt(cmedv)", y="Count")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
```

Based on the table above, when $\lambda = 0.4$, the residuals sum of squared (`RSS`) is the lowest. To be simple, take square root of `cmedv` is approximately following the normal distribution.

### Modeling
Split our Boston data into train set (80%) and test set (20%). Let's build a initial linear regression model $M_1$
$$M_1: E[\sqrt{cmedv}|x] = crim^{-0.12} + \sqrt{indus} + nox^{-1.3} + rm + age^{1.5} + ptratio^{4.5} + lstat^{1/3}$$

The residual plots and fitted value plot help us to detect non-constant variance and non-linearity. If the model is correct, residual plots should expect null plots.
```{r echo=F, warning=FALSE, fig7, fig.align = "center"}
# Set a seed
set.seed(123)

# Randomly split train and test set
index = sample(nrow(data), nrow(data)*0.8)
train = data[index, ]
test = data[-index, ]

# Orginal model
m1 = lm(sqrt(cmedv) ~ I(crim^-0.12) + sqrt(indus) + I(nox^-1.3) + rm + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3), train)

# residual plots
residualPlots(m1)
```


I assume that the constant variance is hold, but there are four regressors (`crim`, `nox`, `rm`, `lstat`) have significantly non-linear, so adding polynomial terms in the models.

Re-fit the linear regression model with second-order terms called $M_2$
$$M_2: E[\sqrt{cmedv}|x] = crim^{-0.12} + crim^{-0.24} + \sqrt{indus} + nox^{-1.3} + nox^{-2.6} + rm + rm^2 +\\ age^{1.5} + ptratio^{4.5} + lstat^{1/3} + lstat^{2/3}$$
```{r echo=F, warning=FALSE, fig8, fig.align = "center"}
# Add second-order term
m2 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(1/nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3), train)

residualPlots(m2)
```


The p-value of `Tukey test` is given 0.48 > 0.05, and most regressors are significantly linear.

Add categorical variables `Tax` and `ANOVA` tests shows p-value
```{r echo=F, warning=FALSE}
# add tax dummy variable
m3 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(tax.new), train)
```

Add categorical variables `Tract` and `ANOVA` tests shows p-value
```{r echo=F, warning=FALSE}
# add tract dummy variable
m4 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(tract.new), train)
```

Add categorical variables `Chas` and `ANOVA` tests shows p-value
```{r echo=F, warning=FALSE}
# add chas dummy variable
m5 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(chas), train)
```

Add categorical variables `zn` and `ANOVA` tests shows p-value
```{r echo=F, warning=FALSE}
# add zn dummy variable
m6 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(zn.new), train)
```

Add categorical variables `rad` and `ANOVA` tests shows p-value
```{r echo=F, warning=FALSE}
# add rad dummy variable
m7 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(rad), train)

# ANOVA test
anova(m2, m3) # not include tax.new
anova(m2, m4) # not include tract.new
anova(m2, m5) # include chas
anova(m2, m6) # not include zn.new
anova(m2, m7) # include rad
```

Based on the `ANOVA` table, the p-value of `chas` is close to zero, so it provides strong evidence to reject the null hypothesis that `chas` is contributed on housing price. Therefore, adding these two categorical regressor into model $M_2$

However, the residuals analysis claims that `crim`, `rm` and `indus` have significantly curvatures, even added polynomial terms. Adding interaction terms with features above, and using `Type II ANOVA` to verify the results.
$$E[\sqrt{cmedv}|x] = crim^{-0.12} + crim^{-0.24} + \sqrt{indus} + indus + nox^{-1.3} + nox^{-2.6} + rm + rm^2 + age^{1.5}\\ + ptratio^{4.5} + lstat^{1/3} + lstat^{2/3} + rm*chas + \sqrt{indus}*chas + crim^{-0.12}*chas + chas $$
```{r echo=F, warning=FALSE}
m8 = lm(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + rm:factor(chas) + sqrt(indus):factor(chas) + I(crim^-0.12):factor(chas) + factor(chas), train)

anova(m8)
```

### Model Selection
Now, use `stepwise` and `best subset selection` with AIC, BIC and mallow's cp criteria to select our models

Choosen the lowest AIC scores, `Model.AIC` shows $R^2 = 0.786$, and all regressors are significantly contributed.
$$Model.AIC = E[\sqrt{cmedv}|x] = rm + rm^2 + age^{1.5} + ptratio^{4.5} + lstat^{1/3} + lstat^{2/3}\\ + nox^{-1.3} + nox^{-2.6} + rm*chas + chas$$
```{r echo=F, warning=FALSE, results='hide'}
m0 = lm(sqrt(cmedv) ~ rm, train)
scope = list(lower=formula(m0), upper=formula(m8))
model.AIC = step(m0, direction="both", scope=scope, k = 2)
```

```{r}
summary(model.AIC)
```


```{r echo=F, warning=FALSE}
model.set = regsubsets(sqrt(cmedv) ~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + rm:factor(chas) + sqrt(indus):factor(chas) + I(crim^-0.12):factor(chas) + factor(chas), train)

model.BIC = lm(sqrt(cmedv) ~ I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(chas), train)
```

Choosen the lowest BIC `r round(summary(model.set)$bic[8],3)` and `Model.BIC` shows $R^2 = 0.781$
$$Model.BIC = E[\sqrt{cmedv}|x] = rm + rm^2 + age^{1.5} + ptratio^{4.5} + lstat^{1/3} + lstat^{2/3} + chas$$
```{r echo=F, warning=FALSE}
summary(model.BIC)
```


Add regularization to perform `Ridge` and `Lasso` models
```{r}
# Lasso
x.train = model.matrix(~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + rm:factor(chas) + sqrt(indus):factor(chas) + I(crim^-0.12):factor(chas) + factor(chas), train)

x.test = model.matrix(~ I(crim^-0.12) + I(crim^-0.24) + sqrt(indus) + I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(age^1.5) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + rm:factor(chas) + sqrt(indus):factor(chas) + I(crim^-0.12):factor(chas) + factor(chas), test)

y.train = sqrt(train[, "cmedv"])
y.test = sqrt(test[, "cmedv"])

model.ridge.cv = cv.glmnet(x=x.train, y=y.train, type.measure = "mse", alpha = 0, nfolds = 10)
model.lasso.cv = cv.glmnet(x=x.train, y=y.train, type.measure = "mse", alpha = 1, nfolds = 10)
```

Use `cross validation` to choose optimal $\lambda$ `r round(model.ridge.cv$lambda.1se,3)` and `r round(model.lasso.cv$lambda.1se,3)`
```{r}
par(mfrow = c(1,2))
plot(model.ridge.cv, main = "Ridge")
plot(model.lasso.cv, main = "Lasso")
```
 
Let's evaluate our four models by square root of mean square error `RMSE`
$$ RMSE = \sqrt{\frac{1}{n}\sum_i^n{(y-\hat{y}_{pred})^2}}$$
### Prediction & Model selection
```{r echo=F, warning=FALSE}
# train.control = trainControl(method = "cv", number = 10)
# model.AIC.CV = train(sqrt(cmedv) ~ rm + I(lstat^1/3) + I(rm^2) + I(ptratio^4.5) +
#     I(lstat^2/3) + factor(chas) + I(age^1.5) + I(nox^-1.3) +
#     I(nox^-2.6) + rm:factor(chas), data = train, method = "lm", trControl = train.control)
# model.AIC.CV
# 
# model.BIC.CV = train(sqrt(cmedv) ~ I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + 
#     I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(chas), data = train, method = "lm", trControl = train.control)
# model.BIC.CV


# cat("RMSE of Ridge:", sqrt(model.ridge.cv$cvm[87]), "\n")
# 
# cat("RMSE of Lasso:", sqrt(model.lasso.cv$cvm[53]))

model.ridge.pred = predict(model.ridge.cv, newx = x.test, type = "response", s = "lambda.1se")
TSS = sum((mean(test$cmedv) - test$cmedv)^2)
RSS.ridge = sum((model.ridge.pred^2 - test$cmedv)^2)
MSE.ridge = mean((model.ridge.pred^2 - test$cmedv)^2)
sqrt(MSE.ridge)
R2.ridge = 1-RSS.ridge/TSS


model.lasso.pred = predict(model.lasso.cv, newx = x.test, type = "response", s = "lambda.1se")
RSS.lasso = sum((model.lasso.pred^2 - test$cmedv)^2)
MSE.lasso = mean((model.lasso.pred^2 - test$cmedv)^2)
sqrt(MSE.lasso)
R2.lasso = 1-RSS.lasso/TSS
```

Based on the `RMSE`, I claim that the model `Model.BIC` is the best fit.
$$Model.BIC = E[\sqrt{cmedv}|x] = rm + rm^2 + ptratio^{4.5} + lstat^{1/3} + lstat^{2/3}\\ + nox^{-1.3} + nox^{-2.6} + chas$$

Show `Model.BIC` performance on test set
```{r}
model.AIC.pred = predict(model.AIC, test, type = "response")
TSS = sum((mean(test$cmedv) - test$cmedv)^2)
RSS.AIC = sum((model.AIC.pred^2 - test$cmedv)^2)
MSE.AIC = mean((model.AIC.pred^2 - test$cmedv)^2)
sqrt(MSE.AIC)
R2.AIC = 1-RSS.AIC/TSS
R2.AIC

model.BIC.pred = predict(model.BIC, test, type = "response")
RSS.BIC = sum((model.BIC.pred^2 - test$cmedv)^2)
MSE.BIC = mean((model.BIC.pred^2 - test$cmedv)^2)
sqrt(MSE.BIC)
R2.BIC = 1-RSS.BIC/TSS
R2.BIC
```

### Model Diagnostics
Next, run model diagnostics to check the assumptions of linear regression that errors have a constant variance, are normally distributed and not independent.

Apply Cook's distance to find outliers and influent points.
```{r echo=F, warning=FALSE, fig10, fig.align = "center"}
influenceIndexPlot(model.BIC, vars=c("Cook","Stud","hat"))
outlierTest(model.BIC, 0.99)
```


Based on the Cook's distance plots and outlier tests, it gives five outliers which have signifianctly evidences. Removed those five outliers and re-fit the `Model.BIC` model.`
```{r echo=F, warning=FALSE}
data.rm = data[!rownames(data) %in% c("153", "215", "365", "369", "370", "371", "372", "373"),]
model.final = lm(sqrt(cmedv) ~ I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + factor(chas), data.rm)

model.final2 = lm(sqrt(cmedv) ~ I(nox^-1.3) + I(nox^-2.6) + rm + I(rm^2) + I(ptratio^4.5) + I(lstat^1/3) + I(lstat^2/3) + I(age^1.5) + factor(chas) + rm*factor(chas), data.rm) 

summary(model.final)
summary(model.final2)
```


Let's create a fitted value plot to evaluate the result
```{r echo=F, warning=FALSE, fig9, fig.align = "center"}
ggplot(train.rm, aes(model.final$fitted.values, cmedv))+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(title = "Fitted Value plot", x = "Predicted Value of cmedv", y = "Actual Value of cmedv")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
```


Make a histgorm and Q-Q plot of residuals to check normality assumption
```{r echo=F, warning=FALSE, fig11, fig.align = "center"}
residual.df = data.frame(residual = resid(model.final))
ggplot(residual.df)+
  geom_histogram(aes(x=residual), bins = 30)+
  labs(title = "Histogram of predicted residuals", x="Residuals", y="Counts")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))

# Q-Q plot
ggplot(residual.df, aes(sample = residual))+
stat_qq()+
stat_qq_line(color = "red")
```


Make residuals plots to check non-constant variance and curvatures
```{r echo=F, warning=FALSE, fig12, fig.align = "center"}
residualPlots(model.final)
```
