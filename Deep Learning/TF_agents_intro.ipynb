{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "# Introduction to Reinforcement Learning with TF-Agents (AML Fall 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKOCZlhUgXVK"
   },
   "source": [
    "This tutorial is adapted from the [official tutorial](https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb) and rewritten for the purpose of the course. Below we will show by examples of how to train reinforcement-learning agents on the CartPole environment using the TF-Agents library.\n",
    "\n",
    "**What is TF-Agents?**\n",
    "\n",
    "TF-Agents is a robust, scalable and easy to use RL Library for TensorFlow. It is still under development, so in this tutorial, we will use the nightly preview build.\n",
    "\n",
    "**Why TF-Agents?**\n",
    "\n",
    " - It is compatible with other TensorFlow high-level API's and therefore is extensively resourceful. \n",
    " - It standardizes common steps of RL and thus helps researchers to try and test new RL algorithms quickly.\n",
    " - It is well tested and easy to configure with gin-config.\n",
    "\n",
    "**Official GitHub Repo:**\n",
    "\n",
    "For the most complete and updated info, please refer to the [official GitHub repo](https://github.com/tensorflow/agents/tree/master/tf_agents).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEHR2Ui-lo8O"
   },
   "outputs": [],
   "source": [
    "# Note: If you haven't installed the following dependencies, run:\n",
    "!apt-get install xvfb\n",
    "!pip install 'gym==0.10.11'\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install PILLOW\n",
    "!pip install 'pyglet==1.3.2'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents-nightly\n",
    "try:\n",
    "  %%tensorflow_version 2.x\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-vcjHEHnIUX"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mUjvFn6nVtX"
   },
   "outputs": [],
   "source": [
    "!pip install tf-agents-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMitx5qSgJk1"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics import py_metric\n",
    "from tf_agents.metrics import tf_py_metric\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NspmzG4nP3b9"
   },
   "outputs": [],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJHVSIr_ntoi"
   },
   "outputs": [],
   "source": [
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NvwH3BmbT7G"
   },
   "source": [
    "# Load the Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
    "\n",
    "In this tutorial, we will use the CartPole-v0 environment from the OpenAI Gym suite.\n",
    "\n",
    "From the [official website](https://gym.openai.com/envs/CartPole-v0/) description of the problem: A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNojfvdiAbAL"
   },
   "outputs": [],
   "source": [
    "# Load the CartPole environment\n",
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "# Inspect the environment\n",
    "print('Observation Spec:\\n', env.time_step_spec().observation)\n",
    "print('Reward Spec:\\n', env.time_step_spec().reward)\n",
    "print('Action Spec:\\n', env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lv8zpzZgZuBF"
   },
   "source": [
    "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n",
    "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n",
    "\n",
    "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TA45WaKAaKaE"
   },
   "source": [
    "In the CartPole environment:\n",
    "\n",
    "-   `observation` is an array of 4 floats: \n",
    "    -   the position and velocity of the cart\n",
    "    -   the angular position and velocity of the pole \n",
    "-   `reward` is a scalar float value\n",
    "-   `action` is a scalar integer with only two possible values:\n",
    "    -   `0` — \"move left\"\n",
    "    -   `1` — \"move right\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4qod_IgaUIS"
   },
   "source": [
    "The CartPole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
    "\n",
    "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BX_Cjz0lEdph"
   },
   "outputs": [],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gonTKKgMTVSX"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0ywbNSUOzej"
   },
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "\n",
    "#@test {\"skip\": true}\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lu_BJ_V9Bg0o"
   },
   "source": [
    "# Define the Policy/ Agent and Start Training\n",
    "\n",
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`.\n",
    "\n",
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FKNFE_u4YWx"
   },
   "source": [
    "## Example 1: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLPuvMjm53Qc"
   },
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4GQ3JKM6J3a"
   },
   "source": [
    "### Define the Policy\n",
    "\n",
    "First, let's try to use the built-in random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tHiYVazYx72"
   },
   "outputs": [],
   "source": [
    "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                            time_step_spec=tf_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJtvOgyzbSwc"
   },
   "source": [
    "### Train Through a Driver\n",
    "\n",
    "In TF-Agents we use a Driver to collect experience in an environment. To use a Driver, we specify an Observer that is a function for the Driver to execute when it receives a trajectory.\n",
    "\n",
    "Thus, to add trajectory elements to the replay buffer, we add an observer that calls `add_batch(items)` to add a (batch of) items on the replay buffer.\n",
    "\n",
    "Using the example below, you should learn how to use the [driver](https://github.com/tensorflow/agents/tree/master/tf_agents/drivers) to control the workflow. Note that the policy in this example is not trained, so there is no RL loops here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RS8vwpDzbVbC"
   },
   "outputs": [],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "ave_return = tf_metrics.AverageReturnMetric()\n",
    "observers = [num_episodes, env_steps, ave_return]\n",
    "\n",
    "# define the driver\n",
    "driver = py_driver.PyDriver(\n",
    "         tf_env, tf_policy, observers, max_steps=200, max_episodes=10)\n",
    "\n",
    "# initial driver.run will reset the environment and initialize the policy.\n",
    "initial_time_step = tf_env.reset()\n",
    "final_time_step, policy_state = driver.run(initial_time_step)\n",
    "\n",
    "print('final_time_step:', final_time_step)\n",
    "print('policy_state:', policy_state)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())\n",
    "print('Average Return: ', ave_return.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YboxwybWTnl0"
   },
   "source": [
    "## Example 2: Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwW7tGkbbdzF"
   },
   "source": [
    "### Same setting, but changing the policy\n",
    "\n",
    "The following example shows how to use the TF-agent policies. Note that this is just for the demonstration of using different existing TF-agent policies. For the Q Learning per se, it actually shouldn't be learned this way. We will show the proper way for Deep Q Learning in later example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Dg1JAWK-VKF"
   },
   "outputs": [],
   "source": [
    "from tf_agents.policies import q_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Wjf04ImUycJ"
   },
   "outputs": [],
   "source": [
    "# Create q network\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(tf_env.observation_spec(),\n",
    "                           tf_env.action_spec(),\n",
    "                           fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOnOadmo9dZM"
   },
   "outputs": [],
   "source": [
    "tf_policy = q_policy.QPolicy(tf_env.time_step_spec(), tf_env.action_spec(),q_network=q_net)\n",
    "\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "metric = tf_metrics.AverageReturnMetric()\n",
    "observers = [num_episodes, env_steps, metric]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=10)\n",
    "\n",
    "# initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())\n",
    "print('Average Return: ', metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUixLdjxcx5F"
   },
   "source": [
    "### Use The Agent Class\n",
    "\n",
    "Below shows a correct way of training a deep q-learning network with `TFUniformReplayBuffer`. We first create an environment, a network and an agent. Then we create a `TFUniformReplayBuffer`. Note that the specs of the trajectory elements in the replay buffer are equal to the agent's collect data spec. We then set it's `add_batch` method as the observer for the driver that will do the data collect during our training. \n",
    "\n",
    "In this example, we use a q network that has a single hidden fully-connected layer of 100 neurons. We first define the network and then pass it to the `dqn_agent.DqnAgent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7W-4eP2ZAlAX"
   },
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    tf_env.time_step_spec().observation,\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=(100,))\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(0.001))\n",
    "\n",
    "replay_buffer_capacity = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eybLnBVHgn1Z"
   },
   "outputs": [],
   "source": [
    "# define the driver\n",
    "def collect_training_data():\n",
    "  dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=1000).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TujU-PMUsKjS"
   },
   "source": [
    "Next, the agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6NWQiEPgzk-"
   },
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "  dataset = replay_buffer.as_dataset(\n",
    "      sample_batch_size=100,\n",
    "      num_steps=2)\n",
    "\n",
    "  iterator = iter(dataset)\n",
    "\n",
    "  loss = None\n",
    "  for _ in range(100):\n",
    "    trajectories, _ = next(iterator)\n",
    "    loss = agent.train(experience=trajectories)\n",
    "    \n",
    "  print('Training loss: ', loss.loss.numpy())\n",
    "  return loss.loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OEuLRinwuqY"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q3yD2DSg2If"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "training_loss = []\n",
    "\n",
    "for i in range(20):\n",
    "  print('Step ', i)\n",
    "  collect_training_data()\n",
    "  training_loss.append(train_agent())\n",
    "\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufpSHdYstGD6"
   },
   "source": [
    "### Evaluate the Agent\n",
    "\n",
    "You may also define your own metric and add it into the observers to inspect. We demonstrate this by reusing the similar codes before, except for adding the maximum reward for evaluating the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2_cgQk9iB6t"
   },
   "outputs": [],
   "source": [
    "class MaxEpisodeScoreMetric(py_metric.PyStepMetric):\n",
    "  def __init__(self, name='MaxEpisodeScoreMetric'):\n",
    "    super(py_metric.PyStepMetric, self).__init__(name)\n",
    "    self.rewards = []\n",
    "    self.discounts = []\n",
    "    self.max_discounted_reward = None\n",
    "    self.reset()\n",
    "  def reset(self):\n",
    "    self.rewards = []\n",
    "    self.discounts = []\n",
    "    self.max_discounted_reward = None\n",
    "  def call(self, trajectory):\n",
    "    self.rewards += trajectory.reward\n",
    "    self.discounts += trajectory.discount\n",
    "    \n",
    "    if(trajectory.is_last()):      \n",
    "      adjusted_discounts = [1.0] + self.discounts # because a step has its value + the discount of the NEXT step (Bellman equation)\n",
    "      adjusted_discounts = adjusted_discounts[:-1] # dropping the discount of the last step because it is not followed by a next step, so the value is useless\n",
    "      discounted_reward = np.sum(np.multiply(self.rewards, adjusted_discounts))\n",
    "      print(self.rewards, adjusted_discounts, discounted_reward)\n",
    "      \n",
    "      if self.max_discounted_reward == None:\n",
    "        self.max_discounted_reward = discounted_reward\n",
    "      \n",
    "      if discounted_reward > self.max_discounted_reward:\n",
    "        self.max_discounted_reward = discounted_reward\n",
    "        \n",
    "      self.rewards = []\n",
    "      self.discounts = []\n",
    "  def result(self):\n",
    "    return self.max_discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du0uC0rdiaR6"
   },
   "outputs": [],
   "source": [
    "class TFMaxEpisodeScoreMetric(tf_py_metric.TFPyMetric):\n",
    "\n",
    "  def __init__(self, name='MaxEpisodeScoreMetric', dtype=tf.float32):\n",
    "    py_metric = MaxEpisodeScoreMetric()\n",
    "\n",
    "    super(TFMaxEpisodeScoreMetric, self).__init__(\n",
    "        py_metric=py_metric, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVE9k-_9tJXj"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent():\n",
    "  max_score = TFMaxEpisodeScoreMetric() \n",
    "  observers = [max_score]\n",
    "  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "      tf_env, agent.policy, observers, num_episodes=100)\n",
    "\n",
    "  final_time_step, policy_state = driver.run()\n",
    "\n",
    "  print('Max test score:', max_score.result().numpy())\n",
    "  return max_score.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_HHrrR1xByw"
   },
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "max_test_score = []\n",
    "\n",
    "for i in range(20):\n",
    "  print('Step ', i)\n",
    "  collect_training_data()\n",
    "  training_loss.append(train_agent())\n",
    "  max_test_score.append(evaluate_agent())\n",
    "\n",
    "plt.plot(np.arange(1, 21, step = 1), training_loss, c = 'orange', label = 'Training loss')\n",
    "plt.plot(np.arange(1, 21, step = 1), max_test_score, c = 'blue', label = 'Max test score')\n",
    "plt.axhline(1.715, c = 'gray', linestyle='dashed', label = 'Max possible score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n",
    "plt.title('Training loss and max test score')\n",
    "plt.xticks(np.arange(1, 21))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF_agents_intro.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
