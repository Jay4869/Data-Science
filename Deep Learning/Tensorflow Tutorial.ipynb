{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial 2.0\n",
    "\n",
    "This notebook is a quick introduction to Tensorflow, from the point of view of using Tensorflow via Python notebooks. Tensorflow is an open source framework for **high performance machine learning**, while you don't need Tensorflow to run a logistic regression (although as we'll see, you can easily do so), you'll need something like it to perform the heavy lifting for the types of models.\n",
    "\n",
    "From a technical standpoint, Tensorflow combines the following:\n",
    "\n",
    "* the ability to define computations in a symbolic manner and extract derivatives automatically by back-propagation\n",
    "* the ability to execute these operations on a variety of hardware, including GPUs and TPUs (GPUs are great for general purpose parallization; TPUs are great at multipling tensors together, which happens a lot when training deep networks)\n",
    "\n",
    "\n",
    "The benefit to you is that this allows you to focus on:\n",
    "\n",
    "* specifying the model (i.e the architecture of your neural network)\n",
    "* specifying the loss (e.g cross entropy for classifcation, squared loss for regression etc)\n",
    "* specifying the mode of training (e.g SGD, Adam etc)\n",
    "\n",
    "rather than the technical aspects of the implementation of the training procedure. I'll also briefly introduce a diagnostic tool called Tensorboard, which will be helpful for examining the progress of the training procedure of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflow graphs\n",
    "\n",
    "In Tensorflow, everything lives in a graph. The neural networks were introduced to you via a graphical representation as a good way of thinking about them conceptually, and this perspective is beneficial from a computational perspective too. However, this means we'll need to write our scripts/programs in a slightly different way then what you'll be used to using vanilla Python.\n",
    "\n",
    "A **computational graph** is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n",
    "\n",
    "- Operations: The nodes of the graph. Operations describe calculations that consume and produce tensors.\n",
    "- Tensors: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return `tf.Tensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_4:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = 2.0 * tf.random_normal(shape=[]) + 3.0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution\n",
    "\n",
    "**The eager execution is the default operating mode of Tensorflow 2.0. So far we have actually been doing everything in eager execution.** TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. The advantages of this approach are easier debugging of all computations, natural control flow using Python statements instead of graph control flow, and an intuitive interface. The downside of eager mode is the reduced performance since graph-level optimizations such as common subexpression elimination and constant-folding are no longer available.\n",
    "\n",
    "While the eager execution mode is easier to code (like plain Python syntax), the graph mode still holds benefits in being portable and highly-performanced. We will not dive deeper into the graph mode in this turorial. If you are interested and wanted to know how to convert to graphical execution in Tensorflow 2.0 via functions, you may refer to [this](https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining a graph and then selecting the subgraph to execute at `sess.run()` time, the exact computation of interest is encapsulated in a Python callable. For example, the program above that uses `sess.run()` to compute `z0` and `z1` can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3c5146a4f919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_z1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'function'"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_z1(x, y):\n",
    "  return tf.add(x, y)\n",
    "\n",
    "@tf.function\n",
    "def compute_z0(x):\n",
    "  return compute_z1(x, tf.square(x))\n",
    "\n",
    "z0 = compute_z0(2.)\n",
    "z1 = compute_z1(2., 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `tf.function` is a decorator that \"defines a TensorFlow function\". **A \"TensorFlow function\" defines a computation as a graph of TensorFlow operations, with named arguments and explicit return values**. Users define the function they want TensorFlow to \"accelerate\" as a Python function and integrate it into their Python program like any other Python function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Tensorflow: Tensors\n",
    "\n",
    "Conceptually, a Tensor is a multi-dimensional data container for modern machine learning. Similar to NumPy ndarray objects, tf.Tensor objects have a data type and a shape. The main difference is that tf.Tensors, additionally, can reside in accelerator memory (like a GPU). TensorFlow offers a rich library of operations (tf.add, tf.matmul, tf.linalg.inv etc.) that consume and produce tf.Tensors.\n",
    "\n",
    "Tensors are characterized by two properties:\n",
    "* datatype - this is usually tf.float32 or tf.int32\n",
    "* shape - this could be e.g $10 \\times 4$, $10 \\times 20 \\times 30$\n",
    "\n",
    "As the image is $100$ pixels by $100$ pixels, we can store this as a $100 \\times 100 \\times 3$ tensor:\n",
    "\n",
    "* the first two dimensions of the array correspond to a $(x, y)$ coordinate of a pixel;\n",
    "* the last dimension corresponds to the Red, Green and Blue channels of the image.\n",
    "\n",
    "In terms of the datatype, this could be either tf.int32 (for example, if the image is 24 bit - so there are 8 bits per channel - then the entries will be an integer between $0$ and $255 = 2^8 - 1$) or a tf.float32 (if we have images of different bit-depths, we will want to normalize them to lie in a range $[0, 1]$ so that our model is agnostic to this).\n",
    "\n",
    "Reference:\n",
    "* https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32\n",
    "* https://www.tensorflow.org/tutorials/customization/basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important types of tensors\n",
    "In Tensorflow, there are several types of tensors which you will come across. The two main ones to keep in mind are:\n",
    "\n",
    "* `tf.Variable` - Variables can be assigned values, and importantly these will be remembered across several calls to session.run. This will be used for the weights and biases in the network. (Why? If you think of one call of session.run as corresponding to one step of the optimization algorithm used to train your neural network, then you'd like to remember the weights/biases after each step!)\n",
    "* `tf.placeholder` - Placeholders are given values only at runtime (when session.run is called), which will allow us to pass arguments to the Tensorflow operations we are about to use. For example, we can then use this to pass the training data into our model so we can then try and fit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow VS Numpy\n",
    "\n",
    "**tf.Tensors work congruently with Numpy arrays (in eager execution)**. NumPy operations accept tf.Tensor arguments. TensorFlow math operations convert Python objects and NumPy arrays to tf.Tensor objects. The tf.Tensor.numpy method returns the object's value as a NumPy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "Tensor shape: (2, 2)\n",
      "Tensor type: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "print('Tensor:', a)\n",
    "print('Tensor shape:', a.shape)\n",
    "print('Tensor type:', a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(25, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting support\n",
    "print(tf.add(1, 2))\n",
    "print(tf.add([1, 2], [3, 4]))\n",
    "print(tf.square(5))\n",
    "print(tf.reduce_sum([1, 2, 3]))\n",
    "\n",
    "# Operator overloading is also supported\n",
    "print(tf.square(2) + tf.square(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from previous examples, tensors and Numpy arrays work congruently with each other. **We will show that we can also initialize a tensor using Numpy.** The function to be used is `tf.convert_to_tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1.3  1.   4.  23.5], shape=(4,), dtype=float64)\n",
      "0: tf.Tensor(1.3, shape=(), dtype=float64)\n",
      "2: tf.Tensor(4.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# create a Python array:\n",
    "array_1d = np.array([1.3, 1, 4.0, 23.5])\n",
    "tf_tensor = tf.convert_to_tensor(value=array_1d, dtype=tf.float64)\n",
    "\n",
    "print(tf_tensor)\n",
    "print('0:',tf_tensor[0])\n",
    "print('2:',tf_tensor[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant/Variable\n",
    "\n",
    "Variables is a primitive class in Tensorflow. It has the following properties:\n",
    "\n",
    "* initial value is required\n",
    "* updated during training\n",
    "* in-memory buffer (saved/restored from disk)\n",
    "* can be shared in a distributed environment\n",
    "* they hold learned parameters of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow variable:\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]])>\n",
      "Numpy variable:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "Tensorflow multiplication:\n",
      "tf.Tensor(\n",
      "[[ 2  4]\n",
      " [ 9 12]], shape=(2, 2), dtype=int32)\n",
      "Numpy muliplication:\n",
      "tf.Tensor(\n",
      "[[ 2  4]\n",
      " [ 9 12]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 2  4]\n",
      " [ 9 12]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Let's try to compare with tf.Variable\n",
    "a = tf.Variable([[1, 2], [3, 4]])\n",
    "b = tf.Variable([[2, 2], [3, 3]])\n",
    "\n",
    "print('Tensorflow variable:')\n",
    "print(a)\n",
    "print('Numpy variable:')\n",
    "print(a.numpy())\n",
    "\n",
    "# tf.multiply of two tf.Variable's \n",
    "print('Tensorflow multiplication:')\n",
    "c_tf = tf.multiply(a, b)\n",
    "print(c_tf)\n",
    "\n",
    "print('Numpy muliplication:')\n",
    "c_np = np.multiply(a, b)\n",
    "print(c_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor: id=243, shape=(2, 2), dtype=int32, numpy=\n",
      "array([[2, 4],\n",
      "       [6, 8]])>\n",
      "  <tf.Tensor: id=246, shape=(2, 2), dtype=int32, numpy=\n",
      "array([[ 3,  6],\n",
      "       [ 9, 12]])>]\n",
      " [<tf.Tensor: id=249, shape=(2, 2), dtype=int32, numpy=\n",
      "array([[ 4,  8],\n",
      "       [12, 16]])>\n",
      "  <tf.Tensor: id=252, shape=(2, 2), dtype=int32, numpy=\n",
      "array([[ 5, 10],\n",
      "       [15, 20]])>]]\n"
     ]
    }
   ],
   "source": [
    "# Don't do this! yields weird answers\n",
    "d = tf.add(a,1)\n",
    "d_np = np.multiply(a, d)\n",
    "print(d_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras and Tensorboard¶\n",
    "One big change in Tensorflow 2.0 is the integration of Keras as a higher level API. The integration makes it easier and faster for new users to get started with TensorFlow. Below we will quickly go through a classification example provided in this post using tf.keras. Then we will implement another model using lower-level Tensorflow without Keras, just to give you a taste.\n",
    "\n",
    "**Quick guide for Tensorboard**\n",
    "\n",
    "Tensorboard is a powerful tool provided by TensorFlow. It allows developers to check their graph and trend of parameters. This guide will give you a basic under standing on how to set up Tensorboard graph in your code, start tensorboard on your local machine/GCP instance and how to access the interface.\n",
    "\n",
    "For complete instructions, check the official guide on Tensorflow web site [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
    "\n",
    "**How to start tensorboard**\n",
    "\n",
    "**Local**\n",
    "\n",
    "To start your Tensorboard on your local machine, you need to specify a log directory for the service to fetch the graph. For example, in your command line, type:\n",
    "\n",
    "```shell\n",
    "$ tensorboard --logdir=\"~/log\"\n",
    "```\n",
    "\n",
    "Then, Tensorboard will start running. By default, it will be running on port 6006:\n",
    "\n",
    "``` shell\n",
    "TensorBoard 1.8.0 at http://localhost:6006 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "Make sure Tensorboard is running, you can visit http://localhost:6006 In your browser and you should be able to see the main page of Tensorboard. If the page is shown as below, it means Tensorboard is running correctly. The report is due to lack of event file, but we can just leave it there for now.\n",
    "\n",
    "![Tensorboard](./notebook_images/tensorboard.png)\n",
    "\n",
    "**GCP**\n",
    "\n",
    "To set up the Tensorboard on GCP is the same as above. However, we're not able to check the Tensorboard UI directly through our browser. In order to visit the page through our local browser, we should link the port of our local machine to the port on GCP. It is similar to what we did previously for Jupyter Notebook.\n",
    "\n",
    "In the command line on your local machine, type:\n",
    "\n",
    "```shell\n",
    "$ gcloud compute ssh --ssh-flag=\"-L 9999:localhost:9999 -L 9998:localhost:6006\" \"ecbm4040@YOUR_INSTANCE\"\n",
    "```\n",
    "\n",
    " This will bind your port of your local machine to the port on GCP instance. In this case, your local port 9999 is binded with 9999 on GCP, while local port 9998 is binded with 6006 on GCP. You can change whatever port you like as long as it does not confilct with your local services.\n",
    "\n",
    "After connecting to GCP using the command, you will be able to see the result page.\n",
    "\n",
    "\n",
    "**Export Tensorboard events into log directory**\n",
    "\n",
    "To generate data files for Tensorboard, we should use class `tf.summary.FileWriter`. This class will save your network graph sturcuture and all the variable summary. The file writer will save the graph and the summary into a directory based on the current timestamp. Here is the code snippet:\n",
    "\n",
    "```python\n",
    "cur_model_name = 'lenet_{}'.format(int(time.time()))\n",
    "# ...\n",
    "\n",
    "# set up summary writer for tensorboard\n",
    "merge = tf.summary.merge_all()\t# merge all the summary for variables for execution\n",
    "writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "```\n",
    "\n",
    "The following code will save all the parameter summary and marked with iteration_total. These data will be displayed in the Tensorboard later on.\n",
    "\n",
    "```python\n",
    "# ... previous code ...\n",
    "# ...\n",
    "\t\t\t\tif iter_total % 100 == 0:\n",
    "                    # do validation\n",
    "                    valid_eve, merge_result = sess.run([eve, merge], feed_dict={xs: X_val, ys: y_val})\n",
    "                    valid_acc = 100 - valid_eve * 100 / y_val.shape[0]\n",
    "                    if verbose:\n",
    "                        print('{}/{} loss: {} validation accuracy : {}%'.format(\n",
    "                            batch_size * (itr + 1),\n",
    "                            X_train.shape[0],\n",
    "                            cur_loss,\n",
    "                            valid_acc))\n",
    "\n",
    "                    # save the merge result summary\n",
    "                    writer.add_summary(merge_result, iter_total)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
