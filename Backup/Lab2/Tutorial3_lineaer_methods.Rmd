---
title: 'Tutorial 4: Linear Methods for Regression and Classification'
author: "Introduction to Statistical Learning with Applications in R"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---


This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

### Simple Linear Regression 

In this example, we are going to use the dataset `state.x77` that comes with standard `R` installation. It is a data set about the 50 states of united states. 

Type `help(state.x77)` in your console window to read more about this data set. 

```{r}
library(datasets)
statedata=as.data.frame(state.x77)
```

First we can modify the column names (the variable names) for easier use. The function `colnames` can be used to retrieve the colume names or assign new names. 

```{r}
colnames(statedata)=c("popu", "inc", "illit", "life.exp", "murder", "hs.grad", "frost", "area")
```

#### Make a scatterplot.

For this example, let's look at the association between life expectancy (`life.exp`) and income (`inc`). The scatterplot below shows a positive association between these two variables. 

```{r}
plot(life.exp~inc, data=statedata)
```

We can compute the correlation between these two variables. The value of the correlation indicates a weak positive linear association. 
```{r}
cor(statedata[,"life.exp"], statedata[,"inc"])
```

There is one observation that is far away from the rest of the points. We would like to know which state is corresponding to that point. Let's add state abbreviations to the plot. 

```{r}
plot(life.exp~inc, data=statedata, type="n")
text(life.exp~inc, data=statedata, state.abb)
```

#### Fit a simple linear regression
Now let's fit the following linear regression model between $Y$ (`life.exp`) and $X$ (`inc`)
$$ Y= \beta_0 +\beta_1 X + \varepsilon. $$

Here $\beta_0$ and $\beta_1$ are the regression coefficients of the model and $\varepsilon$ represents independent random errors. 

Fitting a linear regression is to derive
$$\hat{Y} = b_0 + b_1 X.$$
Here $\hat{Y}$ is a fitted prediction for the observed life expectancies. The difference $Y - \hat{Y}$ is the prediction error, or residual. 

$b_0$ and $b_1$ are estimates for the regression coefficients. They are identified via the least square regression method that minimizes
$$\sum_{i=1}^n (Y - \hat{Y})^2,$$
i.e., the sum of squared prediction errors. 

```{r}
model1=lm(life.exp~inc, data=statedata)
summary(model1)
```

In the above output, the intercept is $b_0$ and the coeffient under the column (`inc`) is $b_1$--the slope. The slope is estimated to be $7.4\times 10^{-4}$. The maganitude of this value does not mean that the effect of income on life expectance is very small. This maganitude is decided by the maganitude of the $X$ variable and $Y$ variable. 

#### Sampling uncertainty in regression analysis

Consider a population of 50 states and we identify the true regression line in this population. Here the function `abline` add a straight line to an existing plot.
```{r}
plot(life.exp~inc, data=statedata,
      xlab="Life Expectancy", ylab="Income")
abline(model1)
```

Now we can consider 4 random samples. We use a [`for` loop](https://en.wikipedia.org/wiki/For_loop) to run an identical regression analysis on 4 randomly selected samples. 

Within the loop, we will implement the following steps for each repetition. 

+ Step 1: randomly select 10 states using the `sample` function of `R`. 
+ Step 2: run least square regression on the selected states only
+ Step 3: compute a 95% confidence band for the true regression line in the population using the sample. 

**[Note]** The states are randomly selected in the following. Therefore, every time you run this file, you will produce different random samples that will give different estimated least regression lines. 

```{r, fig.height=8, fig.width=8}
par(mfrow=c(2,2)) # create a panel of four plotting areas

for(i in 1:4){
  ## Plot the population
  plot(life.exp~inc, data=statedata,
       xlab="Life Expectancy", ylab="Income",
       title=paste("Random sample", format(i)),
       ylim=c(min(life.exp), max(life.exp)+0.3))
  abline(model1)
  if(i==1){
    legend(3030, 74.2, 
           pch=c(NA, NA, NA, 1, 16), 
           lty=c(1, 1, 2, NA, NA),
           col=c(1, 2, 2, 1, 2),
           c("population truth", "sample estimate",
             "sample confidence band", 
             "states", "sampled"),
           cex=0.7,
           bty="n"
           )
  }
  ## Select the sample
  selected.states=sample(1:50, 10)
  points(statedata[selected.states,"inc"], 
         statedata[selected.states,"life.exp"], pch=16, col=2)
  ## Fit a regression line using the sample
  model.sel = lm(life.exp~inc, data=statedata[selected.states,])
  abline(model.sel, col=2)
  ## Make a confidence band. 
  #### first calculate the width of the band, W.
  ww=qt(0.975, 10-2)
  #### generate plotting X values. 
  plot.x<-data.frame(inc=seq(3000, 7000, 1))
  #### se.fit=T is an option to save 
  #### the standard error of the fitted values. 
  plot.fit<-predict(model.sel, plot.x, 
                    level=0.95, interval="confidence", 
                    se.fit=T)

  #### lines is a function to add connected lines 
  #### to an existing plot.
  lines(plot.x$inc, plot.fit$fit[,1]+ww*plot.fit$se.fit, 
        col=2, lty=2)
  lines(plot.x$inc, plot.fit$fit[,1]-ww*plot.fit$se.fit, 
        col=2, lty=2)
}
```

### Multiple Linear Regression

The `MASS` library contains the `Boston` data set, which records `medv` (median house value) for 506 neighborhoods around Boston. We will seek to predict `medv` using 13 predictors such as `rm` (average number of rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).

```{r}
library(MASS) 
library(ISLR)
```

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y∼x1+x2+x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.
```{r}
attach(Boston)
lm.fit=lm(medv ~ lstat+age,data=Boston) 
summary(lm.fit)
```

The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:

```{r}
lm.fit=lm(medv~.,data=Boston) 
summary(lm.fit)
```

What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, `age` has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except `age`.

```{r}
lm.fit1=lm(medv ~.-age,data=Boston) 
summary(lm.fit1)
```

Alternatively, the `update()` function can be used.
```{r}
lm.fit1 = update(lm.fit, .~. -age)
summary(lm.fit1)
```

### Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells `R` to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term lstat×age as predictors; it is a shorthand for `lstat+age+lstat:age`.

```{r}
summary(lm(medv ~ lstat*age,data=Boston))
```

### Non-linear Transformations of the Predictors

```{r}
plot(lstat, medv, pch=16)
```

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula; wrapping as we do allows the standard usage in `R`, which is `I()` to raise `X` to the power 2. We now perform a regression of medv onto `lstat` and `lstat`$^2$.

```{r}
lm.fit2=lm(medv ~ lstat+I(lstat^2)) 
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r}
lm.fit=lm(medv~lstat) 
anova(lm.fit ,lm.fit2)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat`$^2$. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat`$^2$ is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type

```{r}
par(mfrow=c(2,2)) 
plot(lm.fit2)
```

then we see that when the `lstat`$^2$ term is included in the model, there is little discernible pattern in the residuals.

### Qualitative Predictors 

We will now examine the `Carseats` data, which is part of the `ISLR` library. We will attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors.

```{r}
names(Carseats )
```

The `Carseats` data includes qualitative predictors such as `Shelveloc`, an in- dicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor `Shelveloc` takes on three possible values, _Bad_, _Medium_, and _Good_.

Given a qualitative variable such as `Shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.
```{r}
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that `R` uses for the dummy variables.

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

`R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location but lower sales than a good shelving location.

### Logistic Regression
This part, we focus on a classification problem. We will apply logistic regression to the iris data. First, we download the data
```{r}
iris = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", sep = ",", header = FALSE)
names(iris) = c("sepal.length", "sepal.width", "petal.length", "petal.width", "iris.type")
### attach name to each column so that we can directly access each column by its name
attach(iris)
```

We randomly split the data into training set and test set.
```{r}
train = sample.int(nrow(iris), 100)
```


The `glm()` function in `R` can only deal with binary classification problems. Let’s try to distinguish Setosa apart from Virginica, Versicolor.
```{r}
Y = iris.type == "Iris-setosa"
logistic.model = glm(Y ~ sepal.length + sepal.width, data=iris, family = binomial(), subset=train)
logistic.model
```

We can visualize the model in a two-dimensional plot.
```{r}
plot(sepal.length[train], sepal.width[train], type='p',pch=16, col=(Y[train]+4), xlab="Sepal Length", ylab="Sepal Width")
abline(a = -logistic.model$coefficients[1]/logistic.model$coefficients[3], b = -logistic.model$coefficients[2]/logistic.model$coefficients[3], col='gray', lwd=2)
```

To make predictions on the test set, we call the function `predict()`
```{r}
glm.probs = predict(logistic.model, iris[-train,], type="response")
glm.pred = glm.probs>0.5
### summrize the prediction by a confusion matrix
table(Y[-train], glm.pred)
```

### Linear Discriminant Analysis
We can also perfom LDA on the same data set. In `R`, we fit a LDA model using the `lda()` function, which is part of the `MASS` library.  
```{r}
library(MASS)
lda.model<-lda(Y ~ sepal.length + sepal.width, data=iris, subset=train)
lda.model
plot(lda.model)
```

To make predictions, we call the function `predict()`
```{r}
lda.pred = predict(lda.model, iris[-train,])
table(Y[-train], lda.pred$class)
```

For this data set, the LDA and logistic regression predictions are almost identical.