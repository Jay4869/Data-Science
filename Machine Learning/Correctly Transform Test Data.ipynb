{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-use Training Parameters to Transform Test Data\n",
    "\n",
    "Many machine learning algorithms require that features are on the same scale; for example, if we compute distances such as in nearest neighbor algorithms. Also, optimization algorithms such as gradient descent work best if our features are centered at mean zero with a standard deviation of one. For example, the data has the properties of a standard normal distribution. One of the few categories of machine algorithms that are truly scale invariant are the tree-based methods.\n",
    "\n",
    "## Scenario 1\n",
    "\n",
    "scaled_dataset = (dataset - dataset_mean) / dataset_std_deviation\n",
    "\n",
    "train, test = split(scaled_dataset)\n",
    "\n",
    "## Scenario 2\n",
    "\n",
    "train, test = split(dataset)\n",
    "\n",
    "scaled_train =  (train - train_mean) / train_std_deviation\n",
    "\n",
    "scaled_test = (test - test_mean) / test_std_deviation\n",
    "\n",
    "## Scenario 3\n",
    "\n",
    "scaled_train =  (train - train_mean) / train_std_deviation\n",
    "\n",
    "scaled_test = (test - train_mean) / train_std_deviation\n",
    "\n",
    "\n",
    "The correct way is **Scenario 3**! The reason is that we want to pretend that the test data is \"new, unseen data\". We use the test dataset to get a good estimate of how our model performs on any new data. In sum, if we standardize our training dataset, we need to keep the parameters (mean and standard deviation for each feature). Then, we’d use these parameters to transform our test data and any future data later on.\n",
    "\n",
    "In the **Scenario 1**, we need split our data to training set and test set first because we don't want test data to be a part of data processing or model training step, which it makes test data non-generalized to the real world and causes overfitting.\n",
    "\n",
    "In the **Scenario 2**, if you are going to scale test data with itself, then you assume that trianing and test data are following the same distribution, but it might not be true! Therefore, incorrect scaling will effect on the tuning hyper-parameters of models. In a real application, the new, unseen data could be just 1 data point that we want to classify. (How do we estimate mean and standard deviation if we have only 1 data point?) That’s an intuitive case to show why we need to keep and use the training data parameters for scaling the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
