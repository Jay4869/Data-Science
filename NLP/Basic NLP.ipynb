{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Text Process\n",
    "\n",
    "### 1.1. Load Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fishing', 'hiking', 'machinelearning', 'mathematics']\n",
      "./topics/fishing\n",
      "./topics/hiking\n",
      "./topics/machinelearning\n",
      "./topics/mathematics\n"
     ]
    }
   ],
   "source": [
    "path = '../data/Books/hobbies/'\n",
    "folder = os.listdir(path)\n",
    "\n",
    "print(folder)\n",
    "\n",
    "for folder_name in folder:\n",
    "    print(path + folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Local Folder Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body    label\n",
      "0  DNR - Weekly Fishing Report\\nDNR Home Contact ...  fishing\n",
      "1  DNR: Fishing Guide & Regulations\\nÃ Ã Header...  fishing\n",
      "2  NH Hunting and Fishing Licenses | New Hampshir...  fishing\n",
      "3  46 Bait & Tackle - CLOSED - Outdoor Gear - 22 ...  fishing\n",
      "4  5 rescued from capsized sport fishing boat at ...  fishing\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "# loop each sub-folder\n",
    "for folder_name in folder:\n",
    "    \n",
    "    # create full sub-folder path\n",
    "    file_name = os.listdir(path + folder_name)\n",
    "    \n",
    "    # loop each text\n",
    "    for file in file_name:\n",
    "        \n",
    "        # read txt file\n",
    "        f = open(folder_path + folder_name + '/' + file, 'r', encoding='ISO-8859-1')\n",
    "        tmp_read = str(f.read())\n",
    "        \n",
    "        # store dataframe\n",
    "        tmp = pd.DataFrame([tmp_read], columns=['body'])\n",
    "        tmp['label'] = folder_name\n",
    "        data = data.append(tmp, ignore_index=True)\n",
    "        f.close()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "machinelearning    70\n",
       "mathematics        70\n",
       "fishing            64\n",
       "hiking             61\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Lookup Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '#', '#']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "my_string = 'a cat%% jumped# #over #the !!dog&*^'\n",
    "result = re.findall('#', my_string)\n",
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'cat', 'jumped', 'over', 'the', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string_token = word_tokenize(my_new_string)\n",
    "print(string_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Remove Special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned string:  a cat jumped over the dog \n",
      "Cleaned list:  ['a', 'cat', 'jumped', 'over', 'the', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# From string\n",
    "my_new_string = re.sub('[^a-zA-Z0-9]+', ' ', my_string)\n",
    "print('Cleaned string: ', my_new_string)\n",
    "\n",
    "# From list\n",
    "string_token_clean = [word for word in string_token if word.isdigit() or word.isalpha()]\n",
    "print('Cleaned list: ', string_token_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Clean Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords:  ['cat', 'jumped', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_string = [word for word in word_tokenize(my_new_string) if word not in stop_words]\n",
    "\n",
    "print('Remove stopwords: ', clean_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. Porter Stemmer\n",
    "\n",
    "The Porter stemming algorithm (or ‘Porter stemmer’) is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'down', 'laugh', 'the', 'hill', 'ran', 'run']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sentence = 'running downing laughing the hill ran run'\n",
    "my_stemmer = PorterStemmer()\n",
    "\n",
    "[my_stemmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc': 2, 'b': 2, 'c': 4, 'd': 3, 'f': 3, 'g': 2, 'z': 2, 'a': 1, 'e': 1}\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "sentence = 'abc abc B b C c c d F f g z d f g z a e d c'\n",
    "\n",
    "dict_sentence = dict(FreqDist(sentence.lower().split()))\n",
    "print(dict_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'abc', 'b', 'c', 'd', 'e', 'f', 'g', 'z'], dtype='<U3')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sentence.lower().split(), return_counts=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>fence</th>\n",
       "      <th>fox</th>\n",
       "      <th>grass</th>\n",
       "      <th>jumped</th>\n",
       "      <th>on</th>\n",
       "      <th>over</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  fence  fox  grass  jumped  on  over  sat  the\n",
       "0      1      1    1      0       1   0     1    0    2\n",
       "1      1      0    1      2       0   1     0    1    2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc1 = 'the brown fox jumped over the fence'\n",
    "doc2 = 'the brown fox sat on the grass GRASS'\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "my_pd = pd.DataFrame(vector.toarray())\n",
    "my_pd.columns = vectorizer.get_feature_names()\n",
    "\n",
    "my_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. Term Frequency Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>fence</th>\n",
       "      <th>fox</th>\n",
       "      <th>grass</th>\n",
       "      <th>jumped</th>\n",
       "      <th>on</th>\n",
       "      <th>over</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.289569</td>\n",
       "      <td>0.40698</td>\n",
       "      <td>0.289569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236677</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.236677</td>\n",
       "      <td>0.665283</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.332642</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.332642</td>\n",
       "      <td>0.473355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown    fence       fox     grass   jumped        on     over  \\\n",
       "0  0.289569  0.40698  0.289569  0.000000  0.40698  0.000000  0.40698   \n",
       "1  0.236677  0.00000  0.236677  0.665283  0.00000  0.332642  0.00000   \n",
       "\n",
       "        sat       the  \n",
       "0  0.000000  0.579139  \n",
       "1  0.332642  0.473355  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vector = vectorizer.fit_transform([doc1, doc2])\n",
    "my_pd = pd.DataFrame(vector.toarray())\n",
    "my_pd.columns = vectorizer.get_feature_names()\n",
    "\n",
    "my_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11. Part-of-speechTagging (grammatical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taged sentence:  [('patrickran', 'NN'), ('was', 'VBD'), ('running', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('bear', 'NN'), ('in', 'IN'), ('the', 'DT'), ('woods', 'NNS')]\n",
      "selected pos:  ['patrickran', 'running', 'bear', 'woods']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'patrickran was running from the bear in the woods'\n",
    "sentence_tag = nltk.pos_tag(sentence.split())\n",
    "\n",
    "print('taged sentence: ', sentence_tag)\n",
    "\n",
    "valid_pos = ['NN', 'JJ', 'VBG', 'NNS']\n",
    "sentence_clean_pos = [word for word in sentence.split()\n",
    "                     if nltk.pos_tag([word])[0][1] in valid_pos]\n",
    "\n",
    "print('selected pos: ', sentence_clean_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['vs', 'year', 'info', 'new'])\n",
    "\n",
    "folder_path = '../data/Books/hobbies/machinelearning/'\n",
    "directory = os.listdir(folder_path)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in directory:\n",
    "        f = open(folder_path + file, 'r', encoding='ISO-8859-1')\n",
    "        temp = str(f.read())\n",
    "        temp = re.sub('[^a-zA-Z]+', ' ', temp)\n",
    "        temp = [word.lower() for word in temp.split() if word.lower() not in stop_words]    \n",
    "\n",
    "        df = df.append([[temp]])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"maa\" + 0.022*\"mathematics\" + 0.012*\"math\" + 0.008*\"brook\" + 0.008*\"stony\" + 0.006*\"mathematical\" + 0.005*\"university\" + 0.005*\"students\" + 0.004*\"gallery\" + 0.004*\"program\"'),\n",
       " (1,\n",
       "  '0.102*\"university\" + 0.010*\"technology\" + 0.010*\"mathematicians\" + 0.008*\"employment\" + 0.006*\"data\" + 0.006*\"state\" + 0.005*\"science\" + 0.005*\"institute\" + 0.005*\"degree\" + 0.004*\"mathematics\"'),\n",
       " (2,\n",
       "  '0.017*\"mathematics\" + 0.010*\"overview\" + 0.010*\"f\" + 0.007*\"matematika\" + 0.006*\"cambridge\" + 0.005*\"ap\" + 0.005*\"students\" + 0.005*\"published\" + 0.004*\"matem\" + 0.004*\"history\"'),\n",
       " (3,\n",
       "  '0.030*\"list\" + 0.020*\"mathematics\" + 0.017*\"ago\" + 0.013*\"theory\" + 0.011*\"topics\" + 0.009*\"mathematical\" + 0.009*\"mins\" + 0.008*\"named\" + 0.007*\"things\" + 0.006*\"article\"'),\n",
       " (4,\n",
       "  '0.019*\"mathematics\" + 0.016*\"math\" + 0.012*\"research\" + 0.011*\"faculty\" + 0.009*\"graduate\" + 0.009*\"department\" + 0.009*\"students\" + 0.008*\"news\" + 0.007*\"mathematical\" + 0.007*\"university\"')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(df[0])\n",
    "corpus = [id2word.doc2bow(text) for text in df[0]]\n",
    "\n",
    "n_topics = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=n_topics, id2word=id2word, passes=45)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13. NLTK Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk.book\n",
    "\n",
    "books = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "print(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; an\n",
      "Raw Words:  ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]\n",
      "Raw Sentense:  [['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]\n"
     ]
    }
   ],
   "source": [
    "book = 'austen-emma.txt'\n",
    "\n",
    "text = nltk.corpus.gutenberg.raw(book)\n",
    "words = nltk.corpus.gutenberg.words(book)\n",
    "sentenses = nltk.corpus.gutenberg.sents(book)\n",
    "\n",
    "print('Raw Text')\n",
    "print(text[:200])\n",
    "print('Raw Words: ', words[:200])\n",
    "print('Raw Sentense: ', sentenses[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveWord/Sent</th>\n",
       "      <th>Books</th>\n",
       "      <th>BottomWords</th>\n",
       "      <th>Longest_Length</th>\n",
       "      <th>MedWord/Sent</th>\n",
       "      <th>Shortest_Length</th>\n",
       "      <th>TopWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.83</td>\n",
       "      <td>austen-emma.txt</td>\n",
       "      <td>stare,deficiencies,predictions,band,FINIS</td>\n",
       "      <td>274.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to,the,and,of,I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.20</td>\n",
       "      <td>austen-persuasion.txt</td>\n",
       "      <td>defiance,accessions,sunshine,national,Finis</td>\n",
       "      <td>217.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the,to,and,of,a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.33</td>\n",
       "      <td>austen-sense.txt</td>\n",
       "      <td>ranked,disagreement,producing,THE,END</td>\n",
       "      <td>303.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to,the,of,and,her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.57</td>\n",
       "      <td>bible-kjv.txt</td>\n",
       "      <td>sardonyx,chrysolyte,chrysoprasus,transparent,p...</td>\n",
       "      <td>644.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the,and,of,to,And</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.08</td>\n",
       "      <td>blake-poems.txt</td>\n",
       "      <td>Virgin,started,seat,Fled,unhinderd</td>\n",
       "      <td>93.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the,And,and,of,I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveWord/Sent                  Books  \\\n",
       "0         24.83        austen-emma.txt   \n",
       "1         26.20  austen-persuasion.txt   \n",
       "2         28.33       austen-sense.txt   \n",
       "3         33.57          bible-kjv.txt   \n",
       "4         19.08        blake-poems.txt   \n",
       "\n",
       "                                         BottomWords  Longest_Length  \\\n",
       "0          stare,deficiencies,predictions,band,FINIS           274.0   \n",
       "1        defiance,accessions,sunshine,national,Finis           217.0   \n",
       "2              ranked,disagreement,producing,THE,END           303.0   \n",
       "3  sardonyx,chrysolyte,chrysoprasus,transparent,p...           644.0   \n",
       "4                 Virgin,started,seat,Fled,unhinderd            93.0   \n",
       "\n",
       "   MedWord/Sent  Shortest_Length           TopWords  \n",
       "0          17.0              1.0    to,the,and,of,I  \n",
       "1          19.0              1.0    the,to,and,of,a  \n",
       "2          22.0              1.0  to,the,of,and,her  \n",
       "3          28.0              1.0  the,and,of,to,And  \n",
       "4          17.0              1.0   the,And,and,of,I  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import nltk.book\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "summary_book = pd.DataFrame()\n",
    "\n",
    "books = nltk.corpus.gutenberg.fileids()\n",
    "for book in books:\n",
    "    sentenses = nltk.corpus.gutenberg.sents(book)\n",
    "    words = nltk.corpus.gutenberg.words(book)\n",
    "    clean_words = [word for word in words if word.isdigit() or word.isalpha()]\n",
    "    \n",
    "    num_word_sent = np.array([len(sent) for sent in sentenses])\n",
    "    tf = dict(FreqDist(clean_words))\n",
    "    top_five = sorted(tf, key=tf.get, reverse=True)[:5]\n",
    "    top_five = ','.join(top_five)\n",
    "    bot_five = sorted(tf, key=tf.get, reverse=True)[-5:]\n",
    "    bot_five = ','.join(bot_five)\n",
    "    \n",
    "    \n",
    "    summary_book = summary_book.append({'Books': book, \n",
    "                                        'AveWord/Sent': round(np.mean(num_word_sent), 2),\n",
    "                                        'MedWord/Sent': round(np.median(num_word_sent), 2),\n",
    "                                        'Longest_Length': np.max(num_word_sent),\n",
    "                                        'Shortest_Length': np.min(num_word_sent),\n",
    "                                        'TopWords': top_five,\n",
    "                                        'BottomWords': bot_five}, ignore_index=True)\n",
    "\n",
    "summary_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "by              543\n",
       "Jane            199\n",
       "Austen            1\n",
       "VOLUME            3\n",
       "I              2602\n",
       "               ... \n",
       "detailed          1\n",
       "stare             1\n",
       "predictions       1\n",
       "band              1\n",
       "FINIS             1\n",
       "Length: 6224, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict(FreqDist([words for words in raw_text.split() if words.isdigit() or words.isalpha()]))\n",
    "\n",
    "pd.Series(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(x):\n",
    "\n",
    "    tmp = re.sub('[^a-zA-Z]+', ' ', x)\n",
    "    tmp = [my_stemmer.stem(word) for word in tmp.lower().split() if word not in stop_words]\n",
    "    tmp = ' '.join(tmp)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def load_txt(path):\n",
    "    \n",
    "    folder = os.listdir(path)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for folder_name in folder:\n",
    "        \n",
    "        files = os.listdir(path + folder_name)\n",
    "        for txt in files[0:100]:\n",
    "            \n",
    "            f = open(path + folder_name + '/' + txt, \"r\", encoding='ISO-8859-1')\n",
    "            tmp_read = str(f.read())\n",
    "            tmp = pd.DataFrame([clean_up(tmp_read)], columns=['body'])\n",
    "            tmp['label'] = folder_name\n",
    "            df = df.append(tmp, ignore_index=True)\n",
    "            f.close()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_txt('../data/Books/topics/')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abayen</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abe</th>\n",
       "      <th>...</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zimmett</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipcar</th>\n",
       "      <th>ziplin</th>\n",
       "      <th>zmlxpcsgfr</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zucman</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 7475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aac  aaron   ab  abandon  abayen  abbey  abc  abdomen  abe  ...  \\\n",
       "0    0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "1    0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "2    0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "3    0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "4    0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "..   ...  ...    ...  ...      ...     ...    ...  ...      ...  ...  ...   \n",
       "259  0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "260  0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "261  0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "262  0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "263  0.0  0.0    0.0  0.0      0.0     0.0    0.0  0.0      0.0  0.0  ...   \n",
       "\n",
       "     zimmer  zimmett  zip  zipcar  ziplin  zmlxpcsgfr  zone  zoom  zucman  \\\n",
       "0       0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "1       0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "2       0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "3       0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "4       0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "..      ...      ...  ...     ...     ...         ...   ...   ...     ...   \n",
       "259     0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "260     0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "261     0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "262     0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "263     0.0      0.0  0.0     0.0     0.0         0.0   0.0   0.0     0.0   \n",
       "\n",
       "     zumba  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "..     ...  \n",
       "259    0.0  \n",
       "260    0.0  \n",
       "261    0.0  \n",
       "262    0.0  \n",
       "263    0.0  \n",
       "\n",
       "[264 rows x 7475 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf-idf feature matrix creation\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(data.body).toarray()\n",
    "col_names = vectorizer.get_feature_names()\n",
    "data_tfidf = pd.DataFrame(tfidf, columns=col_names)\n",
    "data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 7475)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Variance:  0.9030647825708786\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.181387</td>\n",
       "      <td>-0.100143</td>\n",
       "      <td>0.200938</td>\n",
       "      <td>-0.081673</td>\n",
       "      <td>0.039254</td>\n",
       "      <td>-0.013669</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>0.020803</td>\n",
       "      <td>0.051596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038421</td>\n",
       "      <td>-0.039302</td>\n",
       "      <td>0.031983</td>\n",
       "      <td>-0.038370</td>\n",
       "      <td>0.015136</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>-0.013736</td>\n",
       "      <td>0.089119</td>\n",
       "      <td>0.066295</td>\n",
       "      <td>0.019585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.063698</td>\n",
       "      <td>-0.032335</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>0.035712</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>0.157014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>0.111837</td>\n",
       "      <td>-0.044980</td>\n",
       "      <td>0.034011</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>-0.061929</td>\n",
       "      <td>0.022731</td>\n",
       "      <td>-0.067267</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.015159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.114146</td>\n",
       "      <td>-0.058994</td>\n",
       "      <td>0.115344</td>\n",
       "      <td>-0.025361</td>\n",
       "      <td>-0.008160</td>\n",
       "      <td>0.023572</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>-0.038497</td>\n",
       "      <td>-0.011977</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028164</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.074912</td>\n",
       "      <td>-0.028311</td>\n",
       "      <td>-0.042473</td>\n",
       "      <td>-0.032774</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>-0.038052</td>\n",
       "      <td>-0.084498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.121585</td>\n",
       "      <td>-0.076656</td>\n",
       "      <td>0.131494</td>\n",
       "      <td>-0.045342</td>\n",
       "      <td>-0.034949</td>\n",
       "      <td>0.027078</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>-0.026378</td>\n",
       "      <td>-0.021362</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029123</td>\n",
       "      <td>-0.035085</td>\n",
       "      <td>0.045120</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.001237</td>\n",
       "      <td>-0.019782</td>\n",
       "      <td>0.012156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081335</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.081594</td>\n",
       "      <td>-0.075969</td>\n",
       "      <td>-0.302962</td>\n",
       "      <td>0.578678</td>\n",
       "      <td>0.149967</td>\n",
       "      <td>-0.020976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033049</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>0.090364</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>-0.112766</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>-0.092422</td>\n",
       "      <td>-0.012924</td>\n",
       "      <td>-0.042952</td>\n",
       "      <td>-0.002383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.037523</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>-0.026442</td>\n",
       "      <td>0.056231</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.017048</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>-0.039723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.025752</td>\n",
       "      <td>-0.016650</td>\n",
       "      <td>-0.027616</td>\n",
       "      <td>-0.015633</td>\n",
       "      <td>-0.004495</td>\n",
       "      <td>0.022181</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>-0.008666</td>\n",
       "      <td>-0.010230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.141941</td>\n",
       "      <td>0.963268</td>\n",
       "      <td>0.098784</td>\n",
       "      <td>-0.045705</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.014100</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>-0.015987</td>\n",
       "      <td>-0.022487</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003068</td>\n",
       "      <td>-0.001106</td>\n",
       "      <td>-0.002759</td>\n",
       "      <td>-0.001834</td>\n",
       "      <td>-0.003469</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>-0.002460</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.434892</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>0.051518</td>\n",
       "      <td>-0.137243</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>-0.026002</td>\n",
       "      <td>-0.030151</td>\n",
       "      <td>-0.014079</td>\n",
       "      <td>-0.084019</td>\n",
       "      <td>-0.272144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019421</td>\n",
       "      <td>-0.005590</td>\n",
       "      <td>-0.026385</td>\n",
       "      <td>0.082111</td>\n",
       "      <td>0.092897</td>\n",
       "      <td>0.040259</td>\n",
       "      <td>-0.017303</td>\n",
       "      <td>-0.012520</td>\n",
       "      <td>0.007869</td>\n",
       "      <td>-0.016746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.426131</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>0.048964</td>\n",
       "      <td>-0.097321</td>\n",
       "      <td>-0.049223</td>\n",
       "      <td>0.063168</td>\n",
       "      <td>-0.030065</td>\n",
       "      <td>0.049593</td>\n",
       "      <td>-0.123007</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>0.058438</td>\n",
       "      <td>-0.020102</td>\n",
       "      <td>-0.008643</td>\n",
       "      <td>-0.049832</td>\n",
       "      <td>-0.076775</td>\n",
       "      <td>0.031613</td>\n",
       "      <td>-0.005486</td>\n",
       "      <td>-0.047601</td>\n",
       "      <td>-0.005989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.422472</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>0.032312</td>\n",
       "      <td>-0.020321</td>\n",
       "      <td>-0.185149</td>\n",
       "      <td>0.038919</td>\n",
       "      <td>-0.010215</td>\n",
       "      <td>0.036248</td>\n",
       "      <td>-0.124579</td>\n",
       "      <td>0.012279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038343</td>\n",
       "      <td>0.024034</td>\n",
       "      <td>-0.062660</td>\n",
       "      <td>-0.025214</td>\n",
       "      <td>-0.093508</td>\n",
       "      <td>-0.042061</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>-0.029107</td>\n",
       "      <td>-0.032301</td>\n",
       "      <td>0.066448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.181387 -0.100143  0.200938 -0.081673  0.039254 -0.013669  0.000624   \n",
       "1   -0.063698 -0.032335  0.033231  0.033399  0.035712  0.025147  0.018052   \n",
       "2   -0.114146 -0.058994  0.115344 -0.025361 -0.008160  0.023572  0.008868   \n",
       "3   -0.121585 -0.076656  0.131494 -0.045342 -0.034949  0.027078  0.028620   \n",
       "4   -0.081335  0.005851  0.052194  0.073333  0.081594 -0.075969 -0.302962   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "259  0.037523  0.057915 -0.026442  0.056231  0.002768 -0.010611 -0.001080   \n",
       "260 -0.141941  0.963268  0.098784 -0.045705 -0.018188 -0.014100  0.010137   \n",
       "261  0.434892 -0.000189  0.051518 -0.137243  0.017153 -0.026002 -0.030151   \n",
       "262  0.426131  0.010226  0.048964 -0.097321 -0.049223  0.063168 -0.030065   \n",
       "263  0.422472 -0.002607  0.032312 -0.020321 -0.185149  0.038919 -0.010215   \n",
       "\n",
       "          7         8         9    ...       190       191       192  \\\n",
       "0   -0.023974  0.020803  0.051596  ... -0.038421 -0.039302  0.031983   \n",
       "1    0.002998  0.043760  0.157014  ... -0.022957  0.111837 -0.044980   \n",
       "2   -0.038497 -0.011977  0.009178  ... -0.028164  0.003159  0.074912   \n",
       "3   -0.026378 -0.021362  0.040661  ... -0.029123 -0.035085  0.045120   \n",
       "4    0.578678  0.149967 -0.020976  ...  0.033049  0.020162  0.090364   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "259  0.017048  0.057915 -0.039723  ...  0.006112  0.025752 -0.016650   \n",
       "260 -0.015987 -0.022487 -0.015067  ... -0.003068 -0.001106 -0.002759   \n",
       "261 -0.014079 -0.084019 -0.272144  ... -0.019421 -0.005590 -0.026385   \n",
       "262  0.049593 -0.123007  0.000517  ...  0.010875  0.058438 -0.020102   \n",
       "263  0.036248 -0.124579  0.012279  ... -0.038343  0.024034 -0.062660   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "0   -0.038370  0.015136  0.007553 -0.013736  0.089119  0.066295  0.019585  \n",
       "1    0.034011  0.027529 -0.061929  0.022731 -0.067267  0.001640  0.015159  \n",
       "2   -0.028311 -0.042473 -0.032774  0.014404  0.007798 -0.038052 -0.084498  \n",
       "3    0.008247  0.011360  0.005176 -0.012376 -0.001237 -0.019782  0.012156  \n",
       "4    0.053192 -0.112766  0.041583 -0.092422 -0.012924 -0.042952 -0.002383  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "259 -0.027616 -0.015633 -0.004495  0.022181  0.018584 -0.008666 -0.010230  \n",
       "260 -0.001834 -0.003469 -0.000341  0.000780 -0.002460 -0.000623  0.000753  \n",
       "261  0.082111  0.092897  0.040259 -0.017303 -0.012520  0.007869 -0.016746  \n",
       "262 -0.008643 -0.049832 -0.076775  0.031613 -0.005486 -0.047601 -0.005989  \n",
       "263 -0.025214 -0.093508 -0.042061 -0.032501 -0.029107 -0.032301  0.066448  \n",
       "\n",
       "[264 rows x 200 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=200)\n",
    "pca.fit(data_tfidf)\n",
    "\n",
    "print('Total Variance: ', sum(pca.explained_variance_ratio_))\n",
    "\n",
    "data_pc = pca.transform(data_tfidf)\n",
    "pd.DataFrame(data_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 200)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pc.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
